\documentclass[a4paper,12pt]{article}

\pdfpagewidth 8.5in
\pdfpageheight 11.6in


\setlength{\textwidth}{16.2cm}
\setlength{\textheight}{20cm}
\setlength{\topmargin}{-2cm}
\setlength{\evensidemargin}{-1.5cm}
\setlength{\oddsidemargin}{-1.5cm}

\setlength{\oddsidemargin}{22pt}
\setlength{\topmargin}{22pt}
\setlength{\headheight}{13pt}
\setlength{\headsep}{19pt}
\setlength{\textheight}{630pt}
\setlength{\textwidth}{410pt}
\setlength{\marginparsep}{7pt}
\setlength{\marginparwidth}{56pt}
\setlength{\footskip}{27pt}
\setlength{\marginparpush}{5pt}
\setlength{\hoffset}{0pt}
\setlength{\voffset}{-20pt}
\setlength{\paperwidth}{597pt}
\setlength{\paperheight}{845pt}

% ekezetes betuk bevitele, magyar nyelvi beallaitaok
\usepackage{multirow}
\usepackage{t1enc}
\usepackage[latin2]{inputenc}
\usepackage[magyar]{babel}
\usepackage{fancyhdr}
\usepackage{eucal}
\usepackage{caption}
\usepackage{subcaption}
%\usepackage{setspace}
%\usepackage{ulem}
\usepackage{amssymb}
\usepackage{amsfonts}
%\usepackage{colortbl}
\usepackage{graphicx}%% grafika, abrak
%% kepletek, matek
\usepackage{exscale}
\usepackage{amsmath}
\usepackage{colortbl}
\linespread{1.3}

\newtheorem{lem} {Lemma} [section]
\newtheorem{tet} {Tétel} [section]
\newtheorem{defi} {Definíció} [section]
\newtheorem{axi} {Axióma} [section]
\newtheorem{fel} {Feladat} [section]
\newtheorem{pel} {Példa} [section]
\newtheorem{all} {Állítás} [section]

%bekezdés
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

\def\unitinv{unitér-invariáns}
\def\unitinvanci{unitér-invarianci}

\input{magyarcikk}
\input{matmakro}
\input{grafmakro}

\begin {document}

%fejléc
\begin{titlepage}

\begin{center}
\vspace*{2cm}
 \textsc{\Huge Gráfok spektruma}\textsc{\Large \\[0.8\baselineskip]}\textsc{\Huge}
\par\end{center}{\Huge \par}

\begin{center}
{\Huge \vspace{4cm}
}
\par\end{center}{\Huge \par}

\begin{center}
\textsc{\Large Szakdolgozat\\[0.8\baselineskip] Dudás-Marx László}{\Large{} }
\par\end{center}{\Large \par}

\begin{center}
{\Large \vspace{3cm}
 }\textsc{\Large Témavezetõ: Benczúr A. András Ph.D.}\\
\textsc{\Large{} Operációkutatás tanszék}\\
{\Large{} \vspace{2cm}
 }\textsc{\large Eötvös Loránd Tudományegyetem}\\
\textsc{\large{} Természettudományi Kar}
\par\end{center}{\large \par}

\end{titlepage}

\newpage
\tableofcontents

\newpage

\begin{abstract}
Jelen dolgozat célja, hogy a spektrálgráfelméletbe betekintést nyújtson.
Bemutatjuk a téma elõzményeit, olyan fõbb fogalmait, mint a Laplace-mátrix, a Fiedler-vektor, vagy a Cheeger-konstans. Ezt követõen pedig az ezekhez kapcsolódó fõbb eredményeket. Végül egy valós példán mutatjuk be a sektrálklaszterezés módszer egy lehetséges alkalmazását .

A szakirodalomban bemutatott módszerekhez képest újdonság, hogy a HITS algoritmus segítségével bemutatjuk a sajátvektorok nagy abszolút értékkel reprezentált csúcsai és az utakat koncentráló sûrû részgráfok összefüggéseit.  Valós hálózatokon kísérletekkel igazoljuk, hogy ha egy gráf nem rendelkezik ``nagyléptékû struktúrákkal'', hanem a csúcsok kis-közepes csoportokba rendezõdnek.  Ezeknek a csoportoknak mindegyike egy-egy különbözõ, 1-hez közeli sajátértékû sajátvektorban adja a nagy abszolút értékû helyeket.  Így a spektrum nagyon lassan csökken és nagyon nehéz jó kiegyensúlyozott vágást találni ezekben a gráfokban.
\end{abstract}
\newpage

\section{Bevezetés}
A gráfok különbözõ mátrixainak karakterisztikus függvényeivel, sajátvektoraival és sajátértékeivel a spektálgráfelmélet foglalkozik.
Ez a téma az 1950-es közepétõl vált kutatott területté.
A spektrálgráfelmélet egyik fõ kérdésköre, hogy mit tudunk mondani egy gráf spektrumáról. A másik témakör pedig, hogy hogyan függ össze a spektrum a gráf egyéb tulajdonságaival.

A spektrálgráfelmélet a matematikán belül számos más területhez kapcsolódik. Szoros kapcsolatban van a differenciálgeometriával,a véletlen bolyongásokkal, valamint a Markov-láncokkal is.
A gyakorlatban is sok haszna van a spektrumnak. Elõször a VLSI-áramkörök tervezésénél használták, de azóta számos más területen is sikeresen alkalmazták, mint a fehérjehálózatok, szociális hálók.


\section{Szinguláris felbontás}
Legelõször egy általános keretek közt alkalmazható módszert, a szinguláris felbontást mutatjuk be.

A mátrixok szinguláris felbontásának (\emph{Singular value decomposition}, SVD) létezését már a XIX. század végén bizonyította egymástól függetlenül Beltrami\cite{beltrami} és Jordan\cite{jordan}. A szinguláris felbontást azóta számos fontos területen alkalmazzák. Ilyenek például a homogén lineáris egyenletek megoldása, pszeudoinverz számolása, kis rangú mátrixszal való közelítés, a legkisebb négyzetek módszere, vagy a fõkomponens analízis.

Bár az SVD mátrixok felbontását adja meg, könnyen és gyorsan tudjuk gráfokra is alkalmazni. Ezt úgy tehetjük meg, hogy vesszük a gráf valamelyik mátrix alakú reprezentációját. Egy ilyen alkalmazásra
mutat jó példát majd a HITS-algoritmus.

\subsection{Vektorok és mátrixok normája}
Hogy a szinguláris felbontással kapcsolatos tételeket bizonyítani tudjuk, szükségünk lesz a mátrixnormák, fõként a \emph{Frobenius-norma} ismeretére.

\begin{subequations}
A \lekep[\nu]{\Rnm{n}{m}}{\mathbb{R}} leképezést \emph{mátrixnormának}
nevezzük, ha tetszõleges $A,B \in \Rnm{n}{m}$ mátrixszal és $\alpha \in \mathbb{R}$ számmal  kielégíti az alábbi három összefüggést:
\begin{align*}
A \neq 0      &\kov \nu(A) > 0 \\
\nu(\alpha A) &= |\alpha| \nu(A) \\
\nu(A+B)      &\leq \nu(A) + \nu(B) 
\end{align*}
A mátrixnorma $m=1$ speciális eseteként adódik a \emph{vektornorma}. A következõkben néhány szót ejtünk errõl, azt követõen pedig a mátrixnormákról.

\subsubsection{Vektornormák}

Vektorok \emph{2-normáját} az alábbi egyenlõséggel definiáljuk.
\begin{defin}[2-norma]
 $v\in \mathbb{R}^n$ 2-normája
\[\norma[2]{v} \defegy \sqrt{ \sum_{i=1}^n  |v_i|^2  } = \sqrt{v^{T} v}\,.\]
\end{defin}
Az elsõ két normaaxióma nyilvánvalóan teljesül, a háromszög-egyenlõtlenség pedig a Cauchy-egyenlõtlenség felhasználásával igazolható. A Cauchy-egyenlõtlenség vektorok skaláris szorzata és 2-normája közti kapcsolatot írja le:
\begin{tetel}[Cauchy-egyenlõtlenség] Ha $x,y \in \mathbb{R}^n$, akkor
\[|x^{T} y | \le \norma[2]{x} \norma[2]{y}, \]

ahol egyenlõség pontosan akkor van, ha $x$ és $y$ lineárisan összefüggõk.
\end{tetel}

Vektornormákat más módon is definiálhatnánk, például négyzetgyökvonás és négyzetre emelés helyett tekinthetnénk $1/p$-edik illetve $p$-edik hatványt is. Azonban a 2-normát egy fontos tulajdonság emeli ki a lehetséges vektornormák közül. 
\begin{defin}[Unitér-invariancia vektornormákra]
Egy vektornormát \emph{unitér-invariánsnak} nevezünk, ha tetszõleges $x$ vektor 
és $U$ ortogonális mátrix esetén 
\begin{align*}
\norma{x}&=\norma{Ux}\,.
\end{align*}
\end{defin}
Elõször megmutatjuk, hogy a 2-norma rendelkezik a fenti tulajdonsággal:
$$
 \norma[2]{x}^2=x^{T} x=x^{T} U^{T} U x =(Ux)^{T} (Ux)=\norma[2]{Ux}^2,
$$
tetszõleges $x$ vektorral és $U$ ortogonális mátrixszal, ahonnan adódik az állításunk helyessége. 
Másrészt a fordított irányhoz legyen {\normafv} egy tetszõleges unitér-invariáns norma, és $x_1$ egy 
olyan vektor, melyre $\norma{x_1}=1$. Ekkor az \halmaz{x}{\norma{x}=1} halmaz elemei felírhatóak 
$U x_1$ alakban valamely $U$ ortogonális mátrixszal. Az elõzõ egyenlõséglánc felhasználásával e halmaz 
elemeinek 2-normája megegyezik a $\norma[2]{x_1}$ mennyiséggel. Ezzel igazoltuk a következõ állítást:
\begin{allitas}\label{UnitInvVekt}
Egy {\normafv}  vektornorma pontosan akkor {\unitinv}, ha valamely $c \ge 0$ konstansra 
$\normafv = c \normafv[2]$, azaz ha a 2-norma konstans többszöröse.
\end{allitas}
\subsubsection{Mátrixok normái}
Most áttérünk mátrixok normáinak tárgyalására. Két különbözõ mátrixnormát fogunk bevezetni és használni. Elõször a vektorok 2-normájához hasonló módon definiáljuk az ún. \emph{Frobenius-normát}.
\begin{defin}[Frobenius-norma]
\begin{align*}
\norma[F]{A} &\defegy \sqrt{\sum_{i=1}^{n}  \sum_{j=1}^{m} A_{ij}^2 }\, ,
\end{align*}
\end{defin}

A Frobenius-norma egy a Cauchy-egyenlõtlenséghez hasonló összefüggést is kielégít.
\begin{allitas}\label{FrobKonz}
Tetszõleges $A \in \Rnm{n}{m}, B \in \Rnm{m}{p}$ mátrixok esetén
\begin{align*}
\norma[F]{AB} \leq \norma[F]{A} \norma[F]{B}.
\end{align*}
\end{allitas}
A bizonyítás elemi lépésekkel végezhetõ a Cauchy-egyenlõtlenség felhasználásával. Egy mátrixnormát \emph{konzisztensnek} nevezünk, ha összeszorozható mátrixok és szorzatuk kielégíti a fenti egyenlõtlenséget. Ezzel a tulajdonsággal nem minden mátrixnorma rendelkezik, pedig a konzisztenciára nagy szükség van iterációs algoritmusok hibabecslésénél.
%Ilyen eljárásokban az $n$-edik lépés hibavektorát az $n-1$-edikbõl egy iterációs mátrix szorzásával kapjuk. Ha például az összes iterációs mátrix normája kisebb $\frac{1}{2}$-nél, akkor a norma konzisztenciájából adódik a hiba $(\frac{1}{2})^n$ sebességû lecsengése. 

A második általunk használt mátrixnormát úgy definiáljuk, hogy az konzisztens legyen a vektorok 2-normájával, azaz szeretnénk ha minden $x$ vektorra és $A$ mátrixra teljesülne a következõ egyenlõtlenség: 
\begin{align*}
\norma[2]{Ax} \le \norma{A} \norma[2]{x}.
\end{align*}
A egyenlõtlenségbõl adódik egy alsó becslés \norma{A} értékére. Ezzel az alsó becsléssel definiáljuk mátrixok \emph{2-normáját}, más néven \emph{spektrálnormáját}.
\begin{defin}[Spektrálnorma]
\[\norma[2]{A} \defegy \max_{\norma[2]{x}=1} \norma[2]{Ax}\,.\]
\end{defin}
A képletet röviden úgy értelmezhetjük, hogy egy $A$ mátrix esetén \norma[2]{A} egy $x$ vektor $A$ mátrixszal történõ szorzásával elérhetõ \idez{legnagyobb nyújtás} mértékét határozza meg. A normaaxiómák teljesülése mindkét esetben egyszerûen ellenõrizhetõ a vektorok 2-normájának tulajdonságaiból.

Most néhány olyan összefüggést mutatunk be, melyek szintén meghatározzák az eddig megismert mátrixnormákat. Tetszõleges $A \in \Rnm{n}{m}$ mátrix esetén az $A^{T}A$ mátrix szimmetrikus és pozitív szemidefinit, ezért sajátértékei nem negatív valós számok, melyeket $\lambda_m \ge \dots \ge \lambda_1 \ge 0 $ jelöl. 
\begin{allitas} \begin{eqnarray}
\norma[F]{A}      &=&  \sqrt{ \nyom{A^{T} A} } \label{normaAegyAHegyAllitas} \\
\norma[F]{A}      &=&  \sqrt{ \lambda_1+\lambda_2+\dots +\lambda_m }  \label{FNormaSajatErt}\\
\norma[2]{A}      &=&  \sqrt{\lambda_m} \label{2NormaSajatErt}\\ 
\norma[2]{A}      &=&  \max_{\norma[2]{x}=1,\, \norma[2]{y}=1} |y^{T} A x| \label{MaxyAx}  \\
\norma[2]{A}      &=&  \norma[2]{A^{T}} \label{normaAegyAHegyAT} 
\end{eqnarray}
\end{allitas} 
\begin{biz}
Az $A^{T}A$ mátrix fõátlójában szereplõ $j$-edik elem megegyezik az $A$ mátrix $j$-edik oszlopában elõforduló elemek négyzetösszegével. Így $\nyom{A^{T} A}$ nem más mint $A$ összes elemének négyzetösszege. Ezzel igazoltuk az (\ref{normaAegyAHegyAllitas}) egyenlõséget. 

Az (\ref{FNormaSajatErt}) összefüggés pedig (\ref{normaAegyAHegyAllitas}) következménye, hiszen egy mátrix nyoma megegyezik sajátértékeinek összegével.

Az (\ref{2NormaSajatErt}) összefüggést a következõ módon kapjuk meg:
$$
\norma[2]{A}
=\max_{\norma[2]{x}=1} \norma[2]{Ax}
=\max_{\norma[2]{x}=1} \sqrt{|x^{T}A^{T}Ax| }
=\sqrt{\lambda_1} \,.
$$
A (\ref{MaxyAx}) egyenlõség esetén a $\ge$ irány a Cauchy-egyenlõtlenségbõl adódik, mert 
$$\norma[2]{Ax}=\norma[2]{y} \norma[2]{Ax} \ge |y^{T} A x |\, ,$$
tetszõleges $y$ és $x$ vektorra, melynél $\norma[2]{x}=\norma[2]{y}=1$.
 Legyen $x_1$ az $\norma[2]{A x}$ maximumát beállító vektor, ezt behelyettesítve az elõzõ 
egyenlõtlenségbe kapjuk a $\ge$ irányt. Másrészt $x_1$ és
$y_1 \legy \frac{1}{\norma[2]{A x_1}}A x_1 $ választással
 $$|y_1^T A x_1|=\norma[2]{Ax_1}=\norma[2]{A}$$
adódik, amivel a $\le$ irányt is beláttuk. 
Végül az (\ref{normaAegyAHegyAT}) összefüggés (\ref{MaxyAx}) következménye,
 hiszen a szimmetrikus kifejezésben szereplõ $A$ lecserélhetõ $A^T$-ra.
\end{biz}
%
Mátrixok normájához is társítható a vektorokéhoz hasonló geometriai jelentés.
Tetszõleges $A$ mátrix esetén az 
$$ 
E_A \defegy \halmaz{Ax}{\norma[2]{x}=1} 
$$ 
egy ellipszoidot határoz meg, melynek dimenziója $\rang{A}$. A mátrix normája szemléletesen ennek az  ellipszoidnak a \idez{nagyságát} jelenti.
Az ellipszoidot körbevehetjük egy olyan $\rang{A}$-dimenziós téglatesttel, melynek élei párhuzamosak a fõtengelyekkel, és az élek hossza megegyezik a fõtengelyek hosszával. Ekkor a Frobenius-norma megegyezik a téglatest leghosszabb testátlójával, a 2-norma pedig annak leghosszabb élével vagyis a legnagyobb fõtengely hosszával. Ez (\ref{FNormaSajatErt}) és az (\ref{2NormaSajatErt}) egyenlõségbõl adódik felhasználva, 
hogy az ellipszoid fõtengelyeinek hossza megegyezik $A^{T} A$ sajátértékeivel\cite{Rozsa}. 

Ha egy $A$ mátrixot az $U^T$ és a $V$ ortogonális mátrixokkal szorzunk,  akkor $E_A$-ból forgatással nyerjük az $E_{U^{T} A V}$ ellipszoidot. A mátrix normájától azt várnánk, hogy ne változzon e mûvelet hatására. E tulajdonság azonban nem igaz tetszõleges normára.
\begin{defin}[Unitér-invariania mátrixnormákra]
Egy {\normafv} mátrixnormát \emph{{\unitinv}nak} mondunk, ha tetszõleges $U$ és $V$ ortogonális mátrixokkal
$$ \norma{U^{T} A V} = \norma{A}. $$
\end{defin}
\begin{allitas}
A Frobeinus- és a 2-normák {\unitinv}ak.
\end{allitas}
\begin{biz}
A 2-normára már korábban beláttuk, a Frobenius-normára pedig nyilvánvaló, hogy egy mátrix és 
transzponáltja esetén megegyezik, ezért elég azt megmutatni, hogy egy ortogonális mátrixszal balról szorozva nem változik a norma. Utóbbi a 2-norma esetén abból adódik, hogy a maximalizálandó $\norma[2]{Ax}= \norma[2]{U^{T} Ax} $ bármely $x$ esetén. A Frobenius-norma {\unitinvanci}ája szintén visszavezethetõ vektorok 2-normájáéra, hiszen az $U$ mátrixszal szorozhatunk oszloponként, és a normát is számíthatjuk az oszlopokéból.   
\end{biz}
A dolgozat hátralevõ részében vektorok 2-normájára egyszerûen a \normafv jelölést, mátrixokéra pedig a \normafv[2] jelölést, Frobenius-normára  a \normafv[F] jelölést használjuk. 

\end{subequations}
\subsection{Szinguláris felbontás létezése}
Most már minden eszközünk megvan, hogy a szinguláris felbontást definiáljuk és létezését bizonyítsuk.
\begin{defin}[Szinguláris felbontás]
Egy $A \in \mathbb{R}^{n\times m}$ mátrix szinguláris felbontásán az olyan
\[A = U \Sigma V^T\]
szorzattá bontást értjük, ahol $U \in \mathbb{R}^{n\times n}$ és
$V \in \mathbb{R}^{m\times m}$ ortogonális mátrixok. Továbbá a
$\Sigma\in\mathbb{R}^{n\times m}$, és a ,,fõátlójában'' a
$\sigma_1\geq\cdots\geq\sigma_r>0$- pozitív számokat csupa $0$ követi.
\end{defin}
\begin{defin}[Szinguláris értékek]
A $\sigma_i$ értékeket az $A$ mátrix szinguláris értékeinek hívjuk.
\end{defin}
\begin{defin}[Szinguláris vektorok]
A $U$ és $V$ oszlopait az $A$ mátrix bal, illetve jobboldali szinguláris vektorainak hívjuk.
\end{defin}
\begin{tet}[SVD létezése]
Tetszõleges $A \in \mathbb{R}^{n \times m}$ $r$ rangú mátrixnak létezik szinguláris felbontása.
\end{tet}
\begin{biz}
Teljes indukció $min\left(m,n\right)$-re.
Ha $min\left(m,n\right)=1$, akkor az $A$ mátrix egyedül az $a := A$ vektorból áll. Feltehetjük, hogy $a$ oszlopvektor. $a$-hoz található olyan ortogonális $U$ mátrix, melyre $U^Ta=||a||e_1$. Így $\Sigma = ||a||e_1$ és $V=(1)$ választással adódik a $min\left(m,n\right)=1$ eset.

Ha $min\left(m,n\right) > 1$, akkor a problémát visszavezetjük egy $(n-1)\times(m-1)$ méretû mátrix felbontására. Feltehetõ, hogy $A \neq 0$, különben triviális a felbontás.
\begin{align*}
\sigma_1 &\legy \norma[2]{A}=  \max_{\norma{v}=1} \norma{Av} &
v_1      &\legy \argmax_{\norma{v}=1} \norma{Av} &
u_1      &\legy \frac1\sigma_1 A v_1
\end{align*}
Egészítsük ki továbbá az $u_1$ és $v_1$ vektorokat 
$U_1 \legy (u_1 \, u_2 \, \dots \, u_n)$  és 
$V_1 \legy (v_1 \, v_2 \, \dots \, v_m)$ ortogonális mátrixokká. Ekkor a 
$w ,\, z$ és $A_2$ jelöléseket bevezetve
\begin{align*}
U_1^{T} A V_1 &= \left( \begin{array}{cc}
 \sigma_1 & w^{T} \\
  z       &  A_2
 \end{array} \right)
,&
U_1^{T} A v_1 &= \left( \begin{array}{c}
 \sigma_1  \\
  z       
 \end{array} \right)
,& 
V_1^{T} A^{T} u_1 &= \left( \begin{array}{c}
 \sigma_1 \\
  w
 \end{array} \right).
\end{align*}
\Aref{normaAegyAHegyAllitas}.~állítás szerint 
$\sigma_1=\norma[2]A=\norma[2]{A^{T}}$,  ezért az $\norma{u_1}=\norma{v_1}=1$
 vektorok esetén $\norma{Au_1} \le \sigma_1$ és 
$\norma{A^{T} v_1} \le \sigma_1$. 
Innen a  vektornorma {\unitinvanci}áját felhasználva kapjuk, hogy 
\begin{align*}
\norma{  \left( \begin{array}{c}
 \sigma_1  \\
  z       
 \end{array} \right)}&\le \sigma_1 & 
 &\text{és}&
\norma{  \left( \begin{array}{c}
 \sigma_1  \\
  w      
 \end{array} \right)} &\le  \sigma_1 .       
\end{align*} 
Ez viszont csak úgy lehetséges, ha $z=w=0$, vagyis igazoltuk, hogy 
$$
U_1^{T} A V_1 = \left( \begin{array}{cc}
 \sigma_1 & 0 \\
  0      &  A_2
 \end{array} \right).
$$
Indukciós feltevésünk alapján az $(n-1) \times (m-1)$ méretû $A_2$ mátrixhoz 
találhatók $U_2$ és $V_2$ ortogonális mátrixok, melyekkel $U_2^{T} A_2 V_2 = \Sigma_2$. Az 
\begin{align*}
U^{T} &\legy
\left( \begin{array}{cc}
  1      &  0 \\
  0      &  U_2^{T}
 \end{array} \right) U_1^{T} &
 &\text{és}&
V^{T} &\legy
V_1 \left( \begin{array}{cc}
  1      &  0 \\
  0      &  V_2
 \end{array} \right) 
\end{align*}
választással kapjuk a kívánt felbontását $A$-nak:
$$ 
U^{T} A V= 
\left( \begin{array}{cc}
  \sigma_1      &  0 \\
  0      &  \Sigma_2
 \end{array} \right) 
=\Sigma \,.
$$
\end{biz}
\subsection{HITS-algoritmus}
Jon Kleinberg 1998-ban született HITS(Hyperlink-Induced Topic Search)-algoritmusa az internetes oldalak fontosságát próbálja meghatározni a köztük levõ hiperlinkek alapján.

Az algoritmus ötlete, hogy két fontos csoportba osztja az oldalakat, központi (hub) és mértékadó (authority) oldalakra.
Az algoritmus iteratív módon minden oldalnak frissíti a központiság és mértékadóság mértékét,
azon heurisztika mentén, hogy egy oldal annál központibb, minél mértékadóbb oldalakra mutat, és annál mértékadóbb, minél központibb oldalak mutatnak rá.

A feladat megfogalmazható az irányított gráfok nyelvén is. A weboldalak a csúcsok; az éleket pedig a hiperlinkek határozzák meg, az. $auth,hub:V\rightarrow\mathbb{R}$ függvényeket használjuk majd az egyes oldalak mértékadóságának és központiságának jelzésére.

\subsubsection{Az algoritmus}
Tehát a heurisztika szerint egy oldal központiságát az határozza meg, hogy mennyire mértékadó oldalakra mutat rá.
Tehát a központiság a kiélekbõl jön.

Feltevésünk szerint egy oldal akkor lesz mértékadóbb, ha a központibb oldalak mutatnak rá. Így a mértékadóság a beélekbõl fog származni.

Az algoritmus tehát a következõképpen mûködik.

\textbf{HITS-algoritmus}

\begin{enumerate}
\item Megválasztjuk az $auth$, és $hub$ kezdõértékeket.
\item Frissítjük minden csúcs $auth$ értékekét a beélek mentén a szomszédok aktuális $hub$ értéke szerint.\label{iterLepes}
\[auth^{\left(k+1\right)}\left(v\right)=\sum_{\left(w,v\right)\in E}hub^{\left(k\right)}\left(w\right)\]
\item Frissítjük minden csúcs $hub$ értékét a kiélek mentén a 
szomszédok aktuális $auth$ értéke szerint.
\[hub^{\left(k+1\right)}\left(v\right)=\sum_{\left(v,w\right)\in E}auth^{\left(k+1\right)}\left(w\right)\]
\item Normáljuk az $auth$ és $hub$ értékeket
\item Ellenõrizzük, hogy egy leállási feltétel teljesül-e, ha igen megállunk
\item Visszatérünk a \ref{iterLepes}. pontra
\end{enumerate}

A normalizációs lépés, csak numerikus okok miatt szerepel, mivel a sorrenden nem változtat.
\subsubsection{Az algoritmus elemzése}
%Az algoritmussal kapcsolatban számos kérdés merül fel.
%\begin{itemize}
%\item Konvergál-e az algoritmus?
%\item Milyen értékeket érdemes $auth^{\left(1\right)}$-nak és $hub^{\left(1\right)}$-nak választani?
%\item Hogyan lehet az $auth$ és $hub$ értékeket hatékonyan számolni?
%\end{itemize}

%A kérdések megválaszolását segíti, ha
Az algoritmus egy lépését az adjacencia mátrix segítségével írjuk fel, majd használjuk a szinguláris felbontást.

Készítsünk az $auth$ és $hub$ értékekbõl egy $a$ és $h$ sorvektort!
\[a^{(k)} = (auth^{(k)}(v_1),\dots , auth^{(k)}(v_n));\]
\[h^{(k)} = (hub^{(k)}(v_1),\dots , hub^{(k)}(v_n));\]

Az elõbb definiált vektorokkal és az $A$ adjacenciamátrixszal a következõképp írhatjuk fel az iterációs lépést:
\[a^{\left(k+1\right)}= h^{\left(k\right)}A\]
\[h^{\left(k+1\right)}= a^{\left(k+1\right)}A^T\]
Egymásba helyettesítve a két egyenletet a következõket kapjuk:
\[a^{\left(k+1\right)}=a^{\left(k\right)}A^TA=a^{\left(1\right)}\left(A^TA\right )^k\]
\[h^{\left(k+1\right)}=h^{\left(k\right)}AA^T=h^{\left(1\right)}\left(AA^T\right )^k\]
Alkalmazzuk $A=U\Sigma V^T$ szinguláris felbontását. Mivel $A$ négyzetes mátrix, ezért $\Sigma$ is az, így $\Sigma=\Sigma^T$.
\[a^{\left(k+1\right)}=a^{\left(1\right)}\left(A^TA\right )^k=
a^{\left(1\right)}V\Sigma U^TU\Sigma V^TV\Sigma\dots U\Sigma V^T=
a^{\left(1\right)}V\Sigma^{2k}V^T\]
\begin{equation}
h^{\left(k+1\right)}=h^{\left(1\right)}\left(AA^T\right )^k=
h^{\left(1\right)}U\Sigma V^TV\Sigma U^TU\Sigma\dots V\Sigma U^T=
h^{\left(1\right)}U\Sigma^{2k}U^T \label{hitsEgyenlet}
\end{equation}

Most már jól látható, hogy a mértékadóság és a központiságot meghatározzák a szinguláris értékek és vektorok
Normalizáljuk a $\Sigma$ mátrixot $\sigma_1$-gyel.
\[\Sigma_1 = \left( \begin{array}{cccc}
\frac{\sigma_1}{\sigma_1} & 0 & \dots & 0 \\ 
0 & \frac{\sigma_2}{\sigma_1} & & \vdots\\ 
\vdots & &\ddots & 0  \\ 
0 & \dots & 0 & \frac{\sigma_n}{\sigma_1} \\ 
\end{array}
\right), \quad
\Sigma_1^{2k} = \left( \begin{array}{cccc}
1 & 0 & \dots & 0 \\ 
0 & (\frac{\sigma_2}{\sigma_1})^{2k} & & \vdots\\ 
\vdots & &\ddots & 0  \\ 
0 & \dots & 0 & (\frac{\sigma_n}{\sigma_1})^{2k} \\ 
\end{array}
\right)
\]
Tehát az elsõ elemet kivéve a mátrix összes többi eleme az iterációszámban exponenciális sebességgel tart $0$-hoz.

Ha $h^{(1)}$-t úgy választjuk, hogy merõleges legyen $U$ elsõ oszlopára, akkor ezzel eliminálhatjuk az elsõ szinguláris érték hatását. Ilyenkor a második lesz a hangsúlyos szinguláris vektor.

Tehát a HITS-algoritmus alkalmas a szinguláris felbontás meghatározására.
Ha már ismert az elsõ $k$ szinguláris, akkor választhatunk rájuk merõleges kiinduló vektort. Ekkor a HITS a $k+1$ szinguláris vektorhoz fog konvergálni.

Szemléletes jelentését is társíthatunk az $A^k$, valamint az $(A^TA)^k$ mátrixokhoz.
$a_{ij}$-re felfoghatjuk úgy is, mint az $i$ és $j$ csúcsok közötti $1$ hosszú séták számára. Most nézzük $A^2$-t.
\[A^2_{ij} = \sum_{k=1}^n a_{ik}a_{kj}\]
Könnyen látható, hogy $A^2_{ij}$ az $i$ és $j$ közötti $2$, hosszú séták hosszát adja meg. Indukcióval bizonyíthat, hogy ez magasabb hatványokra is igaz.

$(AA^T)^k$ esetében hasonló eredményre jutunk, csak most minden második lépésünk a megfordított gráfon történik.Látható tehát, hogy az $auth$ és $hub$ értékek is az adott csúcsból induló, és érkezõ oda-vissza utakkal arányos.

Vizsgáljuk meg $A^TA$ hatását a $v{(i)}$ szinguláris vektorokra.
$A^TAv_{(i)}$-t 
\[A^TAv_i=V\Sigma^TU^TU\Sigma V^Tv_i = V\Sigma^2V^Tv_i=V\Sigma^2\left(\begin{array}{c}
0 \\
\vdots \\
1 \\
\vdots \\
0
\end{array}
\right) = 
V\left(\begin{array}{c}
0 \\
\vdots \\
\sigma_i \\
\vdots \\
0
\end{array}
\right) = \sigma_i v_i
\]
Látható, hogy a szinguláris értékek az $A^TA$ sajátértékei és a jobb-szinguláris vektorok a sajátvektorok.

\section{A spektrum}
%TODO valami jobb szöveg ide
Az elõzõ fejezetben láttuk, hogy egy gráf adjacencia mátrixának sajátértékei, spektruma, és sajátvektorai különleges jelentéssel bírnak, és információt hordoznak a gráf struktúrájáról.

A legtöbbször azonban más reprezentánsokat alkalmazunk, a spektrumuk kedvezõbb tulajdonságai miatt. Közös lesz bennük, hogy mindegyik az adjacencia mátrixból gyökerezik.

\subsection{Laplace-mátrix és a normalizált Laplace-mátrix}
Elõször egyszerû gráfokra mutatunk ilyen kedvezõbb mátrixot, az ún. Laplace-mátrixot.
Legyen $G=(V,E)$ egy egyszerû gráf a $|V|=n$ csúcshalmazon, az $|E| \leq \binom{n}{2}$
élhalmazzal! Jelölje $d_v$ a $v$ csúcs fokszámát. Tekintsük a következõ $L$ mátrixot:
\begin{defin}[Laplace-mátrix egyszerû gráfokra]\cite{chung1997spectral}
\[L(u,v)=\left\{
\begin{array}{cc}
d_v & \textrm{ha $u = v$ }\\ 
-1 & \textrm{ha $u$ és $v$ szomszédos} \\ 
0 & \textrm {egyebkent}
\end{array} \right.\]
\end{defin}
Legtöbbször hasonlóság gráfokról mondunk ki állítások. Ezek olyan egyszerû gráfok, melyekhez adva van egy $w:V\times V \rightarrow \mathbb{R}$ súlyfüggvény, amelyre igazak a következõek.
\[w(u,v)=w(v,u)\]
\[w(u,v) \geq 0\]
\[uv \textrm{ nem él} \Leftrightarrow w(u,v)=0\]
A $w(u,v)$ értékeket mátrixba rendezve kapjuk a $W$ mátrixot.
Vegyük észre, hogy a $W$ önmagában meghatározza a gráfot, így a hasonlóság gráfokat a $V$ csúcshalmazzal és a $W\geq 0$ súlymátrixszal fogjuk megadni.

A hasonlóság gráfok esetén szükségünk definiálhatunk egy általánosabb fokszám fogalmat.
\begin{defin}[Általánosított fokszám]
$G=(V,W)$ hasonlóság gráf egy $v$ csúcsának általánosított fokszáma
\[d_v=\sum_{w(u,v) > 0} w(u,v).\]
\end{defin}
Most már definiálhatjuk egy hasonlóság gráf Laplace-mátrixát.
\begin{defin}[Laplace-mátrix]
Egy $G=(V,W)$ hasonlóság gráf Laplace-mátrixa
\[L(u,v)=\left\{
\begin{array}{cc}
d_v-w(u,v) & \textrm{ha $u = v$ }\\ 
-w(u,v) & \textrm{ha $u$ és $v$ szomszédos} \\ 
0 & \textrm {egyebkent}
\end{array} \right.\]
\end{defin}
Jelölje $D\in\mathbb{R}^{n\times n}$ a fokszámmátrixot, vagyis azt a mátrixot, amiben minden fõátlón kívüli elem $0$, a fõátlóban pedig rendre $d_v$ áll.
Ezt a jelölést használva a Laplace-mátrix a következõ formában is felírhatjuk:
\[L=D-W.\]
A Laplace-mátrix már sok elõnyös tulajdonsággal rendelkezik. Szimmetrikus, pozitív-szemidefinit, így a sajátértékei nemnegatív, valósak.
\[\mathcal{L}=D^{-\frac{1}{2}}LD^{-\frac{1}{2}} = I-D^{-\frac{1}{2}}WD^{-\frac{1}{2}}\]
\begin{defin}[Spektrum] A $G=(V,W)$ gráfhoz tartozó $\mathcal{L}$ normalizált Laplace-mátrix sajátértékeit:
\[0=\lambda_0\leq\lambda_1\leq\lambda_2\dots\leq\lambda_{n-1}\]
a $G$ spektrumának hívjuk.
\end{defin}

\subsection{A normalizált Laplace-mátrix néhány egyszerû tulajdonsága}
Mivel $\mathcal{L}$ szimmetrikus ezért az összes sajátérték valós.

Vegyünk egy tetszõleges $g:V\rightarrow\mathbb{R}$ függvényt. És nézzük a $g$-t, mint oszlopvektort. Ekkor tekinthetjük az ún. \textit{Rayleigh-hányadost}
\begin{eqnarray}
%\frac{g^T\mathcal{L}g}{\langle g,g\rangle} =
\frac{\langle g, \mathcal{L}g\rangle}
{\langle g,g\rangle}
=
\frac{\langle g,D^{-\frac{1}{2}}LD^{-\frac{1}{2}}g \rangle}
{\langle g,g \rangle}
=\frac{\langle
f,Lf \rangle}{
\langle
D^{\frac{1}{2}}f,D^{\frac{1}{2}}f 
\rangle
}
=
\frac{\sum_{u \sim v} \left(f\left(u\right)-f\left(v\right)\right)^2w(u,v)}
{\sum_v f\left(v\right)^2 d_v}, \label{Rayleigh}
\end{eqnarray}
ahol $g=D^\frac{1}{2}f$ és $u\sim v$ jelöli azokat a rendezetlen
csúcspárokat, amikre $u$ és $v$ szomszédosak, valamint $\langle ,\rangle$ jelöli a
$\mathbb{R}^n$
-beli belsõszorzatot.

\eqref{Rayleigh}-bõl látszik, hogy az összes sajátérték nemnegatív és a $0$ is sajátérték, mégpedig az $D^-\frac{1}{2}\textbf{1}$ vektorhoz tartozó.
 A $0$-hoz tartozó sajátvektor a
$\sqrt{\underline{d}}=\left(\sqrt{d_1},\dots,\sqrt{d_n}\right)^T$ vektor.
A $0$ akkor és csak akkor egyszeres sajátérték, ha a gráf összefüggõ.
Ha a gráf nem összefüggõ, akkor a spektrum az egyes összefüggõ részek spektrumának uniója.
Ezt abból is láthatjuk, hogy ilyenkor az adjacencia mátrix blokkosított alakra hozható. Ha
$g$ $\mathcal{L}$
sajátvektora, akkor a \textit{Rayleigh-hányados} lesz a hozzá tartozó sajátérték.

Továbbá,
\begin{equation}
\lambda_1 = \inf_{f_\perp D\textbf{1}}
\frac{\sum_{u \sim v} \left(f\left(u\right)-f\left(v\right)\right)^2w(u,v)}
{\sum_v f\left(v\right)^2 d_v} = \min_{\norma{g}=1\atop{g^T\sqrt{\underline{d}} = 0}}
g^T\mathcal{L}g.\label{minLemma}
\end{equation}

Hasonlóképp
\begin{equation}
\lambda_{n-1}=\max_{\norma{g}=1}g^T\mathcal{L}g. \label{maxLemma}
\end{equation}

Az elõzõ $min$ és $max$ egyenlõség hasonlóképp igaz a Laplace-mátrixra és annak sajátértékeire. Érdemes az elõbb vizsgált kifejezést magasabb dimenzióba is kiterjeszteni. 

$X\in \mathbb{R}^{n\times k}$ adott szubortogonális mátrix. $X$ oszlopait az $x_1,\dots,x_k$ vektorokkal, sorai pedig az $r^T_1,\dots,r^T_n$ vektorokkal jelöljük.
\[
Q_k = tr(X^TLX)= \sum_{l=1}^k x_l^T (D-W) x_l =
\sum_{i=1}^n d_i \norma{r_i}^2 - \sum_{i=1}^n \sum_{j=1}^n w_{ij}r_i^Tr_js = \] 
\\
\begin{equation}
\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n w_{ij} \norma{r_i-r_j}^2 \label{Qk}
\end{equation}
Vegyük észre, hogy a $k=1$ esetben az eredeti kifejezést kapjuk vissza.

Tudjuk, hogy egy mátrix nyoma megegyezik a sajátértékeinek összegével. Emiatt
\[\sum_{i=0}^n \lambda_i = tr\left(\mathcal{L}\right) = n .\]
Így ki tudjuk számolni a spektrum átlagát,
amivel becsülni tudjuk a legnagyobb és legkisebb pozitív sajátértéket.
\[\lambda_1 \leq
\frac{\sum_{i=1}^{n-1}\lambda_i}{n-1} = \frac{n}{n-1} \leq
\lambda_{n-1}
\]

Látni fogjuk majd, hogy teljes gráfra egyenlõséggel teljesül mindkét egyenlõtlenég.
Nem teljes gráfra azonban már igaz, hogy $\lambda_1 \leq 1$.

\subsection{Néhány speciális gráf spektruma}
$\mathbf{K_n}.$
Az $n$ csúcsú teljes gráf spektruma szimmetria okokból és mert  $tr \mathcal{L} = n$:
\[\lambda_0 = 0, \lambda_1 = \dots = \lambda_{n-1} = \frac{n}{n-1}.\]
Ebbõl látható, hogy az elõbbi ? becslésünk éles.
$\mathbf{K_{m,n}}.$
Az $m$ és $n$ elemû osztályokból álló teljes páros gráf spektruma:
\[\lambda_0 = 0, \lambda_1 = \dots = \lambda_{n+m-2} =1, \lambda_{n+m-1} = 2.\]
$\mathbf{S_n}.$
Az $n$ ágú, azaz $n+1$ csúcsú csillag spektruma, mivel $S_n = K_{n,1}$:
\[\lambda_0 = 0, \lambda_1 = \dots = \lambda_{n-1} =1, \lambda_{n} = 2 .\]
$\mathbf{P_n}.$
Az $n$ hosszú út spektruma:
\[\lambda_k= 1-\cos\frac{\pi k}{n-1} \quad k=0,\dots,n-1\]
$\mathbf{C_n}.$
Az $n$ hosszú kör spektruma:
\[\lambda_k=1-\cos\frac{2\pi k}{n} \quad k=0,\dots,n-1\] 
$\mathbf{Q_n}.$
A $2^n$ csúcsú $n$ dimenziós hiperkocka sajátértékei:
\[\frac{2k}{n}\quad\binom{n}{k} \textrm{ multiplicitással, } k=0,\dots,n\]

\section{Vágások és partíciók}
Gráfok vizsgálatának egyik fontos kérdése, hogy hogyan tudjuk egy gráf csúcsait jól csoportosítani, partícionálni.
A csoportosítás által egyszerûsödik a gráf szerkezete, jobban átlátható lesz. Csoportosítással rejtett kapcsolatok elõkerülhetnek, amik segíthetik az adott gráf jobb megértését.

\subsection{Vágások}
Gondolhatjuk, hogy két csúcscsoport különbözik, hogyha könnyen el tudjuk választani õket egymástól. Tehát érdemes vizsgálni, hogy hány élet kell elhagyni, hogy  kettéválasszuk õket. Az ilyen élhalmazokat vágásoknak hívjuk.

\begin{defin}[Vágás]
Egy gráf vágása az élek egy olyan részhalmaza, amelyek elhagyás után a gráf már nem lesz összefüggõ.
\end{defin}
$\mathcal{C}(G)$-vel fogjuk jelölni egy $G$ gráf vágásainak halmazát.
Hogy vágásokat össze tudjunk hasonlítani bevezetjük egy vágás értékét.
\begin{defin}[Vágás értéke]
Egy $G=(V,W)$ gráf $C\subset E(G)$ vágásának értéke:
\[w(C)=\sum_{(u,v)\in C}w(u,v),\]
ahol $(u,v)$ rendezetlen párokat jelöl.
\end{defin}

Tetszõleges két csúcshalmazhoz is meghatározhatjuk az általuk feszített részgráfban az elválasztásukhoz szükséges vágás értékét.
\begin{defin}[Vágás értéke]\cite{bolla}
A $G=(V,W)$ élsúlyozott gráfhoz tartozó $S,T \in V$ nem üres csúcshalmazok által meghatározott vágás súlya
\[w(S,T)=\sum_{s\in S}\sum_{t\in T}w_{st}\]
\end{defin}
A definíciót kiterjeszthetjük tetszõleges számú halmazra.
\begin{defin}[k-vágás értéke]
\[cut(P_k,G) = sum_{a=1}^{k-1}\sum_{b=a+1}^{k}w(V_a,V_b)\]
\end{defin}

Láttuk, hogy a spektrum erõs összefüggésben van a gráfok szerkezetével, így nem meglepõ, hogy a minimális vágás sem független a spektrumtól. Felhívnánk a figyelmet, hogy a következõ állításban a sima Laplace és nem a normált spektrumot használjuk fel. 
\begin{allitas} Az $n$ csúcsú $\wg$ összefüggõ gráf $L$ Laplace-mátrixának sajátértékeit jelölje 
Legyenek a $0=\lambda_0<\lambda_1\leq \dots \leq \lambda_{n-1}$.
\[\min_{C\in\mathcal{C}(G)}w(C) \geq \frac{n-1}{n	}\lambda_1\]
\end{allitas}

\begin{biz}

Legyen $U^*$ egy min-vágás. Az $U$ segítségével definiáljuk a következõ egydimenziós reprezentánsokat.
\[r_i= \left\{ \begin{array}{cc}
n-|U^*| & \textrm{ha } i \in U^* \\
-|U^*| & \textrm{ha } i \in \overline{U^*}
\end{array}
\right.
\]
Legyen $\textbf{r}$ az $r_i$ értékeket összefogó vektor. Normáljuk $\textbf{r}$-t.
\[\norma{\textbf{r}} = \sum_{i=1}^n r_i^2 =|U^*|(n-|U^*|)^2 +
(n-|U^*|)|U^*|^2 = |U^*|(n-|U^*|)n
\]
\[\tilde{\textbf{r}} = \frac{\textbf{r}}{\sqrt{|U^*|(n-|U^*|)n}}\]
Az \ref{maxLemma} lemma és \ref{Qk} %TODO
alapján:
\[\lambda_{1}\leq \tilde{\textbf{r}}^TL\tilde{\textbf{r}} = \sum_{i<j}w_{ij}(\tilde{r}_i-\tilde{r}_j)^2 =
w(U^*,\overline{U^*})\frac{(n-|U^*|+|U^*|)^2}{|U^*|(n-|U^*|)n}\]
Ebbõl következik, hogy
\[\min_{C\in\mathcal{C}(G)}w(C) \geq \frac{|U^*|(n-|U^*|)}{n}\lambda_{1}\geq\frac{n-1}{n}\lambda_{1}\]
\end{biz}

Az elõzõ bizonyítás menetét követve könnyen adódik az alábbi állítás is.
\begin{allitas}[Max-vágás súlya]
Legyenek a $0=\lambda_0\leq\lambda_1\dots\leq\lambda_{n-1}$ a $G=(V,W)$ gráfhoz tartozó $L$ Laplace-mátrix sajátértékei. Ekkor
\[ \max_{U\in V}w(U,\bar{U}) \leq \frac{n}{4}\lambda_{n-1}.\]
\end{allitas}
\begin{biz}
Az elõzõ állítás bizonyítását, csak annyiban módosítjuk, hogy a (\ref{maxLemma}) lemmát használjuk, valamint, hogy nem minimális, hanem maximális vágást határozzon meg $U^*$.
Így kapjuk végül a kívánt egyenlõtlenséget.
\[\max_{C\in\mathcal{C}(G)}w(C) \leq \frac{|U^*|(n-|U^*|)}{n}\lambda_{n-1}\leq\frac{n}{4}\lambda_{n-1}\]
\end{biz}
%TODO szöveg
\begin{defin}[Fiedler-vektor]
A $\wg$ gráf normalizált Laplace-mátrixának legkisebb pozitív sajátértékhez tartozó
sajátvektort Fiedler-vektornak hívjuk.
\end{defin}

Fiedler a $\lambda_1$ sajátértékét algebrai összefüggõségnek nevezte.

\subsection{Izoperimetrikus feladatok}
Kézenfekvõ, hogy minimális vágással osszuk részekre a csúcshalmazt.
Azonban könnyen elõfordulhat, hogy egy minimális vágás csak egy csúcsot vág le a gráfról, valójában pedig nem ez a ,,legjobb'' vágás.

\begin{figure}[h]
\centering
\includegraphics[scale=1]{pictures/mincut.jpg} 
\caption{Példa arra, amikor a minimális vágás nem optimális}
\end{figure}

Tehát keresnünk kell valamilyen más mértéket, ami mentén optimalizálunk.
Azokat a problémákat, ahol optimális vágásokat keresünk a halmazok méretének figyelembe vételével,
izoperimetrikus problémáknak is nevezik.

Mivel célunk, hogy az egy partícióba tartozó csúcsok hasonlóak legyenek, célul tûzhetjük ki, hogy az egy partícióba levõ csúcsok ne legyenek túl messze egymástól. Vagyis, hogy az egyes részek átmérõje ne legyen túl nagy.

\begin{defin}[Átmérõ]
Egy gráf átmérõjén, a csúcsai között fellépõ távolságok maximumát értjük.   
\[D(G) = \max_{u,v\in V(G)} dist(u,v)\]
\end{defin}
Tehát van értelme nézni a következõ feladatot:
\[\min_{P_k\in \mathcal{P}_k}\max_{V_i \in P_k} D(V_i).\]
Könnyen tudunk azonban olyan példát mutatni, amire nem a kívánt partíciókat kapjuk.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.1]{pictures/diameter-fail.jpg}
\caption{Példa arra, amikor a max. átmérõ nem ad optimális vágást}
\end{figure}

\newpage

Szeretnénk megtartani a minimum vágás azon tulajdonságát, hogy a komponensek közötti élek összsúlya kicsi.
Így mértékek meghatározásához érdemes tehát a minimum vágásból kiindulni, és normálni a csúcshalmazok valamilyen mértékével.

Legegyszerûbb a csúcshalmazok méretével normálni, így kaphatjuk a kiegyensúlyozottság mértékét.
\begin{defin}[Kiegyensúlyozottság]
\[b(P_k, G) = \sum_{a=1}^{k-1} \sum_{b=a+1}^{k} \left (\frac{1}{|V_a|} + 
\frac{1}{|V_b|}\right )w(V_a,V_b)=\]
\[=\sum_{a=1}^k\frac{w(V_a, \overline{V_a})}{|V_a|} =
 k-\sum_{a=1}^k\frac{w(V_a,V_a)}{|V_a|}\]
\end{defin}

Normálhatunk az illeszkedõ élek súlyával is.
A késõbb is hasznos lesz a következõ fogalom.

\begin{defin}[Csúcshalmaz térfogata]
A $\graf$ gráf $S\subset V$ csúcshalmazának térfogata:
\[Vol S = \sum_{v\in S}d_v.\]
\end{defin}

Shi és Malik \cite{shi2000normalized} a következõket mértéket javasolja. 

\begin{defin}[Normalizált k-vágás érték]
\[Ncut(P_k, G)=\sum_{a=1}^{k-1}\sum_{b=a+1}^{k}
\left (\frac{1}{Vol(V_a)}+\frac{1}{Vol(V_b)}\right )w(V_a,V_b)=\]
\[=\sum_{a=1}^k\frac{w(V_a, \overline{V_a})}{Vol(V_a)} =
 k-\sum_{a=1}^k\frac{w(V_a,\overline{V_a})}{Vol(V_a)}\]
\end{defin}

A kiterjesztés már közelebb áll az általunk elvárt mértékhez.
\begin{defin}[Kiterjesztés]
\[\Psi(S)=\frac{w(S,\overline{S})}{\min(|S|, |\overline{S}|)}\]
\end{defin}

A kiterjesztés hiányossága, hogy minden csúcsot egyformán súlyoz, ezt kijavíthatjuk, ha nem elemszámot, hanem térfogatot veszünk. Így a konduktancia fogalmát kapjuk.

\begin{defin}[Konduktancia]
\[\Phi(S)=\frac{w(S,\overline{S})}{\min(Vol(S), Vol(\overline{S}))}\]
\end{defin}

\subsection{Bipartíciók}
Elõször csak azzal a kérdéssel foglalkozunk, amikor csak két részre szeretnénk osztani $G$ csúcsait.
Vagyis bipartícionálni szeretnénk $G$-t.
Egy bipartíció rögtön meghatároz egy vágást is.
\[ \partial S = \big\{ \{ u,v \} \in E(G): u \in S , v \notin S \big\} \]
Világos, hogy $\partial S = \partial \overline{S}$.
%A vágás éleinek nem $S$-ben levõ végpontjait $\delta S$-el jelöljük majd.
%\[\delta S = \{v \notin S : \{u, v\} \in E(G), u \in S\}\]


\subsubsection{A Cheeger-konstans}
Tekintsük a következõ két feladatot!

\textbf{1. feladat:} Egy fix $m$-re keressük azt az $S$ csúcshalmazt,
amire $m\leq Vol(S)\leq Vol(\overline{S})$ és $w(\partial S)$ minimális

Mindkettõre majd az ún. \textit{Cheeger-konstans} adja meg a választ.

\begin{defin}[Cheeger-konstans]
Egy $\graf$-hoz tartozó Cheeger-konstans
\[h_G=\min_{S\subset V}\Phi\left(S\right).\]
\end{defin}
$h_G > 0$ csak akkor igaz, ha $G$ összefüggõ, ezért feltesszük, hogy $G$ összefüggõ.

Az 1. feladat megoldása egyenértékû a Cheeger-konstans meghatározásával, mivel
\[w(\partial S)\geq h_G Vol(S)\]


\subsubsection{A Cheeger-egyenlõtlenség}
A következõkben a Cheeger-konstans és a spektrum kapcsolatát fogjuk vizsgálni.
\begin{tetel}[Cheeger-egyenlõtlenség]
Egy $\wg$ összefüggõ élsúlyozott gráfhoz tartozó $h_G$ Cheeger-konstansra és a, spektrumának legkisebb pozitív sajátértékére igazak az alábbi egyenlõtlenségek:
\[\frac{h_G^2}{2}<\lambda_1 \leq 2 h_G\].
\end{tetel}
\begin{biz}
\textbf{Felsõ becslés:} Legyen $S\subset V$ Cheeger-optimális. vagyis $h_G\left( S\right) = h_G$. Tekintsük a következõ $f:V\rightarrow\mathbb{R}$ függvényt:
\[ f\left(v \right) =
\left\{
\begin{array}{cc}
\frac{1}{Vol (S)}&\textrm{ha } v \in S\\
\frac{1}{Vol( \overline{S})}& \textrm{ha } v \in \bar{S}\\
\end{array}
\right.
\]
$f$-t behelyettesítve \eqref{Rayleigh}-ba kapjuk a következõt:
\[
\lambda_1 \leq \partial S\left(\frac{1}{Vol(S)}+\frac{1}{Vol(\overline{S})}\right) \leq
\frac{2\partial S}{\min(Vol (S), Vol (\overline{S}))} =
2 h_G
\]
\end{biz}
A másik irányt most csak abban az eseteben látjuk be, amikor nem súlyozottak az élek.

\begin{biz}[Felsõ becslés]
Nézzük a $\lambda_1$-hez tartozó egységhosszú sajátvektort, $f$-t és $g=D^{-1/2}f$  vektorokat,
mint $V\rightarrow \mathbb{R}$ függvényeket. Mivel $f$ merõleges a $\sqrt{\underline{d}}$-re, ezért igaz, hogy
\begin{equation}
\label{eq:gd0}
\sum_v g(v)d_v=0.
\end{equation}
%\[\sum_v g(v)^2 d_v=1.\]
Feltehetõ, hogy a csúcsok $f$ szerint csökkenõ sorrendbe vannak rendezve vagyis $f(v_i) \geq f(v_j)\textrm{, ha } i<j$. Legyen $S_i=\{v_1,\dots, v_i\}$, valamint \[\alpha_G=\min_i h_{S_i}.\].
Legyen $r$ a legnagyobb egész, amire $Vol(S_r)<Vol(G)/2$. \eqref{eq:gd0} miatt
\[\sum_v g(v)^2d_v=\min_c\sum_v(g(v)-c)^2 d_v\leq \sum_v(g(v)-g(v_r))^2d_v.\]
Jelölje $g_+$ és $g_-$ $g$ pozitív ill. negatív részét.
\[\lambda_1 = \frac{\sum_{u\sim v}(g(u)-g(v)^2)}{\sum_v g(v)^2d_v}
\geq \frac{\sum_{u\sim v}(g(u)-g(v)^2)}{\sum_v (g(v)-g(v_r))^2d_v}\]
\[\geq\frac{\sum_{u\sim v}\Big(\big(g_+(u)-g_+(v))^2 + (g_-(u)-g_-(v)\big)^2\Big)}
{\sum_v(g_+(v)^2+g_-(v)^2)d_v}\]
Mivel
$
\frac{a+b}{c+d} \geq min(\frac{a}{c},\frac{b}{d})
$, feltehetõ, hogy
\[\lambda_1 \geq \frac{\sum_{u\sim v}(g_+(u)-g_+(v)^2)}{\sum_v g_+(v)^2d_v}\]

\[=\frac{\Big(\sum_{u\sim v}\big(g_+(u)-g_+(v)\big)^2\Big)
\Big(\sum_{u\sim v}\big(g_+(u)+g_+(v)\big)^2\Big)}
{\Big(\sum_v g_+(v)^2d_v\Big)\Big(\sum_{u\sim v}\big(g_+(u)+g_+(v)\big)^2\Big)}\]

\[\geq \frac{\Big(\sum_{u\sim v}\big(g_+(u)^2-g_+(v)^2\big)\Big)^2}{2\left (\sum_v g_+^2(v)d_v\right )^2}\textrm{, a Cauchy-Schwarz-Bunyakovszkij egyenlõtlenség miatt,}\]
\[=\frac{\Big(\sum_i |g_+(v_i)^2-g_+(v_{i+1})^2|\,|\partial(S_i)|\Big)^2}{2\Big(\sum_v g_+(v)^2d_v\Big)^2}\]
Bevezetve a
\[\tilde{Vol}(S)=\min(Vol(S), Vol(G)-Vol(S))\] 
jelölést kapjuk továbbá, hogy
\[\lambda_1 \geq \frac{\Big(\sum_i |g_+(v_i)^2-g_+(v_{i+1})^2|\alpha_G|\tilde{Vol}(S_i)|\Big)^2}{\Big(\sum_v g_+(v)^2d_v\Big)^2}\textrm{, }\alpha_g\textrm{ definíciója miatt}\]
\[=\frac{\alpha_g^2}{2}\frac{\Big(\sum_i g_+(v_i)^2\big(|\tilde{Vol}(S_i)-\tilde{Vol}(S_{i+1})|\big)\Big)^2}{\Big(\sum_v g_+(v)^2d_v\Big)^2}\]
\[=\frac{\alpha_g^2}{2}\frac{\Big(\sum_ig_+(v_i)^2d_{v_i}\big)^2}{\Big(\sum_v g_+(v)^2d_v\Big)^2} = \frac{\alpha_G^2}{2}\]

Beláttuk tehát, hogy $\lambda_1 \geq h_G^2/2$. Ahhoz, hogy belássuk, hogy egyenlõség nem állhat fent szükségünk lesz arra, hogy $1-\sqrt{1-x^2}>x^2/2$, ha $x > 0$, valamint az alábbi lemmára.
\begin{lemma}
Ha $G$ összefüggõ gráf és $\lambda_1$ a spektrumának legkisebb eleme, akkor
\[\lambda_1\geq 1-\sqrt{1-h_G^2} \Rightarrow 
\]
\end{lemma}
\begin{biz}
Láttuk, hogy 
\[\lambda_1 \geq \frac{\sum_{u\sim v}(g_+(u)-g_+(v)^2)}{\sum_v g_+(v)^2d_v} = W.\]
Valamint azt is tudjuk már, hogy
\[W =\frac{\Big(\sum_{u\sim v}\big(g_+(u)-g_+(v)\big)^2\Big)
\Big(\sum_{u\sim v}\big(g_+(u)+g_+(v)\big)^2\Big)}
{\Big(\sum_v g_+(v)^2d_v\Big)\Big(\sum_{u\sim v}\big(g_+(u)+g_+(v)\big)^2\Big)}\]
\[\geq \frac{\Big(\sum_{u\sim v}|g_+(u)^2-g_+(v)^2|\Big)^2}
{\Big(\sum_vg_+(v)d_v\Big)\Big(2\sum_vg_+(v)^2d_v-W\sum_vg_+(v)^2d_v\Big)}\]
\[\geq\frac{\Big(\sum_i |g_+(v_i)^2-g_+(v_{i+1})^2|\,|\partial S_i|\Big)^2}
{(2-W)\Big(\sum_v g_+(v)^2\Big)^2d_v}\]
\[\geq\frac{\Big(\sum_i\big(g_+(v_i)^2-g_+(v_{i+1})^2\big)\alpha_G\sum_{j\leq i}d_j\Big)^2}
{(2-W)\Big(\sum_v g_+(v)^2\Big)^2d_v}\]
\[
\geq \frac{\alpha_G^2}{2-W}
\]
Ebbõl következik, hogy
\[0\geq W^2-2W+\alpha_G^2.\]
A másodfokú egyenlet megoldóképletét alkalmazva kapjuk a kívánt egyenlõtlenséget.
\[\lambda_1 \geq W \geq 1-\sqrt{1-\alpha_G^2} \geq 1 - \sqrt{1-h_G^2}\]
\end{biz}
A lemma bizonyításával kész vagyunk a Cheeger-egyenlõtlenség bizonyításával.
\end{biz}

A $k$-dimenziós kocka Cheeger-konstansa $2/n$, ugyanannyi, mint a spektrumának $\lambda_1$ eleme. Tehát a Cheeger-egyenlõtlenség felsõ becslése konstans szorzó erejéig pontos. \\
A $n$-hosszú út spektrumának elsõ pozitív eleme $1-\cos\frac{\pi}{n-1}\approx\pi^2/2(n-1)^2$,
a Cheeger-konstansa pedig $1/\lfloor \frac{n-1}{2} \rfloor$.
Megmutattuk tehát, hogy a Cheeger egyenlõtlenség alsó iránya is a lehetõ legjobb becslés konstans szorzó erejéig.

%TODO Fiedler-vektort ide

\subsection{Klaszterezés}
A gráfok klaszterei olyan csúcshalmazok, amelyek valamilyen
jellemzõ tulajdonság alapján mérhetõen elkülönülnek.
Ilyenek lehetnek a szociális hálókban az egy nyelvet beszélõk, fehérjehálózatokban egyes csoportok. %TODO ide valami jobb példa

A klaszterektõl elvárjuk, hogy az egy csoportban levõ elemek hasonlítsanak egymásra, míg a különbözõ klaszterekben levõk különbözzenek.
Valamint elvárt az is, hogy a klaszterek hasonló méretûek legyenek.
Klaszterezés során minden csúcsot be kell osztanunk egy klaszterbe.
Tehát a klaszterezés egy $k$-részre vágás, vagyis a $G=(V,W)$ gráf csúcsainak egy olyan $P_k=(V_1,\dots,V_k)$ felosztása, ahol $V_i$-k diszjunktak és \[\bigcup_{i=1}^k V_i =V\].


\subsubsection{Reprezentánsok keresése QP programmal}

%TODO szöveg

Nézzük a következõ kvadratikus programozási feladatot.

Adott $\wg$ gráfhoz és adott $1\leq k \leq n$ egészhez keresünk olyan $k$-dimenziós $r_1, \dots r_n$ reprezentáns vektorokat, hogy a 
\begin{equation}
Q_k = \sum_{i < j} w_{ij} \norma{r_i - r_j} \geq 0 \label{celfuggveny}
\end{equation}
célfüggvény érték minimális legyen a
\begin{equation}
\sum_{i=1}^n r_ir_i^T = I_k \label{feltetelek}
\end{equation}
feltétel mellett ($I_k$ jelöli a $k$ dimenziós egységmátrixot).
Látható, hogy a célfüggvény értéke csökken, ha a nagy hasonlóságú csúcsok közelebb kerülnek egymáshoz.

Átalakítjuk a célfüggvényt, és a feltéteket egy többet mondó formába.
Legyen $X$ az a mátrix, aminek a sorai $r_1^T,\dots ,r_n^T$. Legyenek $x_1, \dots, x_k \in \mathbb{R}^n$ $X$ oszlopai.
A \eqref{feltetelek} feltételek miatt $X$ oszlopai ortonormált rendszert alkotnak, tehát $X$ egy szubortogonális mátrix.
Így a feltételeket felírhatjuk $X^TX=I_k$ formában is. Ezek után a célfüggvényt tovább alakíthatjuk (\ref{Qk}) egyenlet szerint.
\begin{eqnarray}
\min \nyom{X^TLX} \label{minQk} \\
X^TX=I_k
\end{eqnarray}

\begin{tetel}[Reprezentációs tétel hasonlóság gráfokra]
Tekintsük a $G=(V,W)$ összefüggõ gráfot és a hozzá tartozó $L$ Laplace-mátrixot!
Jelölje $0=\lambda_0\leq\lambda_1\leq\cdots\leq\lambda_{n-1}$ az $L$ sajátértékeit,
valamint $u_0,u_1,\dots,u_{n-1}$ a hozzájuk tartozó
egységhosszú sajátvektorokat! Legyen $k<n$ olyan,
hogy $\lambda_{k-1}<\lambda_k$!
Ekkor a \eqref{celfuggveny} célfüggvény értéke a \eqref{feltetelek} feltételek mellett:
\[\sum_{i=0}^{k-1}\lambda_i=\sum_{i=1}^{k-1}\lambda_i.\]
És az optimális reprezentánsok $r_1^*,\dots,r_n^*$ az $X^*=\left(u_0, u_1, \dots ,u_{k-1}\right)$ mátrix sorainak transzponáltjai.
\end{tetel}
\begin{biz}
A \eqref{minQk} alak szerint:
\[ Q_k = \nyom{X^TLX}\]
Így a tétel következik az alábbi általánosabb állításból.
\begin{allitas}
Legyen $A$ egy valós $n\times n$ méretû szimmetrikus mátrix. $A$ sajátértékei $\lambda_1\geq\dots\geq\lambda_n$.
$k > 0$ olyan, hogy $\lambda_k > \lambda{k+1}$ Ekkor
\[
\max_{X\in\mathbb{R}^{n\times k}\atop{ X^TX=I_k}} \nyom{X^TAX}=
\max_{x_i \in \mathbb{R}^n \left( i=1,\dots ,k \right)\atop{ x_i^Tx_j=\delta_{ij}}}
\sum_{i=1}^k x_i^T A x_i = \sum_{i=1}^k\lambda_i
\]
És az optimum az $X=\left(u_1,\dots,u_k\right)$ szubortogonális mátrix, ahol $u_i$ az $\lambda_i$-hez tartozó egységhosszú sajátvektor. 
\end{allitas}
\end{biz}

A klaszterek meghatározásához alkalmazhatunk egy általános geometriai klaszterezõ algoritmust a jóval kisebb dimenziós reprezentáns vektorokon.
Most bemutatjuk a legismertebb ilyen klaszterezõ algoritmus a k-means.

\textbf{k-means algoritmus}

Adott $l$ db $\mathbb{R}^n$-beli vektor, és $i$ iterációszám.
\begin{enumerate}
\item Választunk $k$ db tetszõleges klaszterközéppontot, $\emph{számláló} := 0$
\item Minden csúcsot beosztunk abba a klaszterbe, amelyiknek a középpontja legközelebb esik hozzá.
\item Minden klaszter középpontja legyen a benne levõ pontok átlaga
\item Ha $\emph{számláló} = i$, akkor megállunk, egyébként visszalépünk 2. pontra és \emph{számláló}-t növeljük 1-gyel. 
\end{enumerate}

\subsubsection{Spektrálklaszterezés}
Már láttuk, hogy a Fiedler-vektor segítségével hogyan határozhatunk meg vágásokat.
Klaszterezés esetén alkalmazhatjuk szubrutinként a vágási eljárást.

\textbf{Hagen algoritmusa}
\begin{enumerate} 	\label{spektralHeurisztika}
\item Vegyük a $\lambda_1$-hez tartozó sajátvektort
\item A Fiedler-vektor szerint sorba rendezzük a csúcsokat
\item Megkeressük a minimális konduktanciájú vágást a sorrendben
\item A két részre külön-külön alkalmazzuk az algoritmust
\end{enumerate}

Kannan, Vempala és Vetta becslést is adtak \cite{kannan2004clusterings} az
elõzõ algoritmus által talált klaszterek minõségére. A becsléshez az alábbi mérték használták.

\begin{defin}[$(\alpha , \epsilon )$-partíció]
Egy $\{V_1, \dots, V_k\}$ partíció $(\alpha , \epsilon )$-partíció, ha
\begin{itemize}
\item $C_i$ konduktanciája legalább $\alpha$
\item $i\neq j,$ $C_i$ és $C_j$ között menõ élek súlya az összes él súlyának legfeljebb $\epsilon$ része.
\end{itemize}
\end{defin}


\begin{allitas}[Kannan-Vempala-Vetta]
Ha a $G$ gráfnak létezik $(\alpha , \epsilon )$-partíciója,
akkor a \ref{spektralHeurisztika} spektrálheurisztika talál
\[\left (\frac{\alpha^2}{72\log^2\frac{n}{\epsilon}},
20\sqrt{\epsilon}\log\frac{n}{\epsilon}\right )-\textrm{partíciót}.\]
\end{allitas}

\subsection{Spektrálklaszterezés több sajátvektorral}
Az elõzõ részben látott algoritmus hátránya, hogy nem stabil, és nem hatékony.
Shi és Malik \cite{shi2000normalized} egy másik megközelítést javasol a spektrálklaszterezésre. Nem csak a Fiedler-vektort, hanem több sajátvektort is érdemes figyelembe venni.
Az õáltaluk használt algoritmus a következõ.

\textbf{Shi és Malik algoritmusa}
\begin{enumerate}
\item Keressük meg a normalizált Laplace-mátrix elsõ $k$ pozitív sajátértékéhez tartozó sajátvektort
\item A vektorokból képzett mátrix sorait klaszterezzük pl. $k-means$ algoritmussal
\item A $v$ csúcs a $v$-nek megfelelõ sor klaszterébe kerül.
\end{enumerate}

Shi és Malik megmutatta, hogy minél több sajátértéket veszünk, annál jobban közelítjük az optimális normalizált k-vágást. Brand és Huang megmutatta \cite{brand2003unifying}, hogy több sajátvektort véve a klaszterek eloszlása egyenletesebb lesz, és a
hasonló pontok távolsága csökken, a különbözõké pedig növekszik.

\section{Spektrálklaszeterezés a gyakorlatban}
A következõ fejezetben a spektrálklaszterezés gyakorlatban való alkalmazhatóságát vizsgálnánk meg.
A gyakorlatban azt tapasztaljuk, hogy a nagy gráfokon a spektrumon alapuló algoritmusok gyengébb eredményeket érnek el, mint más megközelítést használó módszerek.
Ennek okait szeretnénk bemutatni, valamint lehetséges megoldásokat bemutatni egy konkrét példa.
\subsection{A spektrálklaszterzés gyengeségei}
\subsubsection{A TKC-hatás}
Lempel és Moran az internetgráf vizsgálatakor találták meg a HITS algoritmus egy hibáját. Azt vették észre, hogy a HITS-algoritmus gyakran felértékel a valóságban nem mértékadó oldalakat. 
A hiba okának a kis sûrû részgráfokat azonosították. Ezért a
jelenséget TKC-hatásnak{tightly kit communities effect} nevezték el.


\begin{lemma}
\label{lemma:eigenvector}
Legyen $S = U \Lambda U^T$ szimmetrikus pozitív szemidefinit mátrix, ahol
\[
\Lambda = \left(
\begin{array}{ccc}
\lambda_0 & \ldots & 0 \\
0 & \ddots & 0 \\
0 & \ldots & \lambda_{n-1}
\end{array}
\right).
\]

Ha az $x$ kiinduló vektor merõleges $u_1,\ldots,u_{i-1}$-re, de nem merõleges $u_i$-re,  és $\lambda-{i+1} < \lambda_i$, akkor

\begin{equation}
\lim_{k\mapsto\infty} \frac{S^k x}{\lambda_i^k} = u_i.
\end{equation}
\end{lemma}

\begin{biz} A HITS-nél látott \eqref{hitsEgyenlet} egyenlethez hasonlóan $S^k u_i = \lambda_i^k u_i$. Az  állítás következik, ha felírjuk $x$-t az $u_i$-k, mint bázisok által meghatározott térben.
\end{biz}

\begin{lemma}
\label{lemma:path-count}
Legyen $S$ egy súlyozott gráf  élmátrixa. Ekkor $S^k x$ az $x$-el súlyozott kezdõcsúcsokból induló $k$ hosszú séták  összsúlya.
\end{lemma}

A fenti két lemmát alkalmazhatjuk a HITS algoritmusnál látott $S=AA^T$ mátrixokra, a Laplace, illetve a súlyozott Laplace mátrixokra is. A \ref{lemma:path-count}.~Lemma szerint azokra a $j$ csúcsokra lesz magas az $[S^k x]_j$  értéke, amelyekbõl nagyon sok súlyozott út indul. A \ref{lemma:eigenvector}.~Lemma szerint ezeknek a $j$ csúcsoknak lesz kiemelkedõ  értéke az elsõ olyan sajátvektorban, amely $x$-re nem merõleges.

\subsection{Lehetséges módszerek a TKC-hatás megoldására}
Ebben a részben néhány heurisztikát szeretnénk ismertetni,
melyek segítségével javítható a spektrálklaszterezés eredménye.
\subsubsection{Csomópontok eltávolítása}
A TKC-hatást okozó csomópontok megtalálására alkalmazhatjuk a SCAN-algoritmust\cite{xu2007scan}. Ha megtaláltuk a gráf méretéhez képest elhanyagolható csomópontokat, azokat elhagyhatjuk.
\subsubsection{Csápok összehúzása}
Csápoknak neveztük egy elõre rögzített $d_t$ fokszámnál kisebb fokú csúcsokat. Ezek eltávolítására a következõ eljárást alkalmazhatjuk.
\begin{enumerate}
\item{Keresünk egy $v$ csúcsot, amire $d_v < d_t$, ha nincs ilyen, akkor megállunk.}
\item{$v$-t összehúzzuk a legnagyobb fokszámú szomszédjával.}
\item{Felírjuk, hogy melyik két csúcsot húztuk össze.}
\item{Újra kezdjük az 1. lépéstõl}
\end{enumerate}
Az eljárás véges, mert minden lépésben csökken a csúcsszám.

\subsection{Spektrálklaszterezés egy konkrét példán bemutatva}
Most pedig egy valós példán demonstráljuk, hogy hogyan alkalmazható a spektrálklaszterezés a gyakorlatban.
\subsubsection{Az adat: livejournal.com}
Példánkban\cite{kurucz} a LiveJournal(www.livejournal.com) oldal felhasználóit szerették volna klaszterezni.
A LiveJournal egy közösségi blog oldal, vagyis a felhasználók nem csak tartalmakat közölhetnek, hanem lehetõségük van egymással való kapcsolatukat (barátság) is bejelölni. A kapcsolat szimmetrikus, így a szociális háló élei irányítatlanok.

Célkitûzés volt, hogy az orosz nyelvû felhasználókat geológia elhelyezkedésük szerint szeparálják. Az orosz nyelvû felhasználók jelentõs része valójában valamelyik környezõ ország lakosa(Ukrajna, Fehéroroszország, Észtország,\dots).

A gráfban 2.379.267 csúcs és 14.286.827 él szerepelt. A felhasználók több mint 80\% amerikai és körülbelül 5\% orosz.

\subsubsection{A kísérlet menete}
A gráfból elõször eltávolították a csomópontokat, majd alkalmazták a csápok összehúzását. Innen kétféleképp haladtak tovább. Egyrészt Shi és Malik algoritmusát követve, másrészt Lang megmutatta, hogy az optimális klaszterezésnek létezik szemidefinit relaxáltja \cite{lang2005fixing}.
Végül visszarakták a kivett csomókat, és a csápokat visszanövesztve kapták a végsõ klasztereket. A spektrálklaszterezést lefuttatták elõfeldolgozás nélkül is, az összehasonlításhoz. 
\subsubsection{Eredmények}
Az eredményeket az alábbi ábrák mutatják be.

\begin{figure}[ht]
\centering
\begin{subfigure}[b]{0.4\textwidth}
	\includegraphics[width=\textwidth]{pictures/klaszter_before.jpg} 
	\caption{Elõfeldolgozás nélkül}
\end{subfigure}
\begin{subfigure}[b]{0.4\textwidth}
	\includegraphics[width=\textwidth]{pictures/klaszter_after.jpg}
	\caption{Elõfeldolgozással}
\end{subfigure}
\caption{A csúcsok eloszlása 4. és 5. sajátirányok mentén}
\end{figure}

Látható, hogy , míg az elõfeldolgozás nélkül a két csoportot nem lehet megkülönböztetni. A heurisztikák alkalmazása után a klaszterek eltávolodnak egymástól.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{pictures/sdp_result.jpg}
\caption{A szemidefinit program kimenete}
\end{figure}

A szemidefinit programmal összehasonlítható lett az eredmény, míg a spektrálklaszterezés sokkal gyorsabban futott.

\section{Összefoglalás}

\bibliographystyle{huplain}

\bibliography{hivatkozasok}

\end{document} 
