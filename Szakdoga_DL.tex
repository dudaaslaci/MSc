\documentclass[a4paper,12pt]{article}

\pdfpagewidth 8.5in
\pdfpageheight 11.6in


\setlength{\textwidth}{16.2cm}
\setlength{\textheight}{20cm}
\setlength{\topmargin}{-2cm}
\setlength{\evensidemargin}{-1.5cm}
\setlength{\oddsidemargin}{-1.5cm}

\setlength{\oddsidemargin}{22pt}
\setlength{\topmargin}{22pt}
\setlength{\headheight}{13pt}
\setlength{\headsep}{19pt}
\setlength{\textheight}{630pt}
\setlength{\textwidth}{410pt}
\setlength{\marginparsep}{7pt}
\setlength{\marginparwidth}{56pt}
\setlength{\footskip}{27pt}
\setlength{\marginparpush}{5pt}
\setlength{\hoffset}{0pt}
\setlength{\voffset}{-20pt}
\setlength{\paperwidth}{597pt}
\setlength{\paperheight}{845pt}

% ekezetes betuk bevitele, magyar nyelvi beallaitaok
\usepackage{multirow}
\usepackage{t1enc}
\usepackage[latin2]{inputenc}
\usepackage[magyar]{babel}
\usepackage{fancyhdr}
\usepackage{eucal}
\usepackage{caption}
\usepackage{subcaption}
%\usepackage{setspace}
%\usepackage{ulem}
\usepackage{amssymb}
\usepackage{amsfonts}
%\usepackage{colortbl}
\usepackage{graphicx}%% grafika, abrak
%% kepletek, matek
\usepackage{exscale}
\usepackage{amsmath}
\usepackage{colortbl}
\linespread{1.3}

\newtheorem{lem} {Lemma} [section]
\newtheorem{tet} {Tétel} [section]
\newtheorem{defi} {Definíció} [section]
\newtheorem{axi} {Axióma} [section]
\newtheorem{fel} {Feladat} [section]
\newtheorem{pel} {Példa} [section]
\newtheorem{all} {Állítás} [section]

%bekezdés
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

\def\unitinv{unitér-invariáns}
\def\unitinvanci{unitér-invarianci}

\input{magyarcikk}
\input{matmakro}
\input{grafmakro}

\begin {document}

%fejléc
\begin{titlepage}

\begin{center}
\vspace*{2cm}
 \textsc{\Huge Gráfok spektruma}\textsc{\Large \\[0.8\baselineskip]}\textsc{\Huge}
\par\end{center}{\Huge \par}

\begin{center}
{\Huge \vspace{4cm}
}
\par\end{center}{\Huge \par}

\begin{center}
\textsc{\Large Szakdolgozat\\[0.8\baselineskip] Dudás-Marx László}{\Large{} }
\par\end{center}{\Large \par}

\begin{center}
{\Large \vspace{3cm}
 }\textsc{\Large Témavezetõ: Benczúr A. András Ph.D.}\\
\textsc{\Large{} Operációkutatás tanszék}\\
{\Large{} \vspace{2cm}
 }\textsc{\large Eötvös Loránd Tudományegyetem}\\
\textsc{\large{} Természettudományi Kar}
\par\end{center}{\large \par}

\end{titlepage}

\newpage
\tableofcontents

\newpage

\begin{abstract}
Jelen dolgozat célja, hogy a spektrálgráfelméletbe betekintést nyújtson.
Bemutatjuk a téma elõzményeit, olyan fõbb fogalmait, mint a Laplace-mátrix, a Fiedler-vektor, vagy a Cheeger-konstans. 
%Végül egy valós példán mutatjuk be egy lehetséges alkalmazását a módszernek.

A szakirodalomban bemutatott módszerekhez képest újdonság, hogy a HITS algoritmus segítségével bemutatjuk a sajátvektorok nagy abszolút értékkel reprezentált csúcsai és az utakat koncentráló sûrû részgráfok összefüggéseit.  Valós hálózatokon kísérletekkel igazoljuk, hogy ha egy gráf nem rendelkezik ``nagyléptékû struktúrákkal'', hanem a csúcsok kis-közepes csoportokba rendezõdnek.  Ezeknek a csoportoknak mindegyike egy-egy különbözõ, 1-hez közeli sajátértékû sajátvektorban adja a nagy abszolút értékû helyeket.  Így a spektrum nagyon lassan csökken és nagyon nehéz jó kiegyensúlyozott vágást találni ezekben a gráfokban.
\end{abstract}
\newpage

\section{Bevezetés}
A gráfok különbözõ mátrixainak karakterisztikus függvényeivel, sajátvektoraival és sajátértékeivel a spektálgráfelmélet foglalkozik.
Ez a téma az 1950-es közepétõl vált kutatott területté.
A spektrálgráfelmélet egyik fõ kérdésköre, hogy mit tudunk mondani egy gráf spektrumáról. A másik témakör pedig, hogy hogyan függ össze a spektrum a gráf egyéb tulajdonságaival.

A spektrálgráfelmélet a matematikán belül számos más területhez kapcsolódik. Szoros kapcsolatban van a differenciálgeometriával, a véletlen bolyongásokkal, valamint a Markov-láncokkal is.
A gyakorlatban is sok haszna van a spektrumnak. Elõször a VLSI-áramkörök tervezésénél használták, de azóta számos más területen is sikeresen alkalmazták, mint a fehérjehálózatok, szociális hálók struktúrájának felderítése, vagy képszegmentáció.

A dolgozat elején egy általánosabb eszközt, a mátrixok szinguláris felbontását mutatjuk be. Bizonyítjuk ennek létezését, valamint megmutatjuk Kleinberg HITS-algoritmusával való kapcsolatát. Ezt követõen definiáljuk a legfontosabb fogalmakat, mint egy gráf Laplace-mátrixa, annak súlyozott változata és utóbbi sajátértékei, a spektrum. Bemutatjuk a spektrummal kapcsolatos alapösszefüggéseket. A következõ fejezetben a spektrumot különbözõ bi- és multipartíciók értékeinek becslésére, és maguknak az optimálist közelítõ partícióknak a meghatározására használjuk. A dolgozat végén egy valós példán mutatjuk be az elmélet egy lehetséges gyakorlati alkalmazását.

\section{Szinguláris felbontás}
Legelõször egy általános keretek közt alkalmazható módszert, a szinguláris felbontást szeretnénk bemutatni.

A mátrixok szinguláris felbontásának \emph{(Singular value decomposition, SVD)} létezését már a XIX. század végén bizonyította egymástól függetlenül Beltrami \cite{beltrami} és Jordan \cite{jordan}. A szinguláris felbontást azóta számos fontos területen alkalmazzák. Ilyenek például a homogén lineáris egyenletek megoldása, pszeudoinverz számolása, kis rangú mátrixszal való közelítés, a legkisebb négyzetek módszere, vagy a fõkomponens analízis.

Bár az SVD mátrixok felbontását adja meg, könnyen és gyorsan tudjuk gráfokra is alkalmazni. Ezt úgy tehetjük meg, hogy vesszük a gráf valamelyik mátrix alakú reprezentációját. Egy ilyen alkalmazásra
mutat jó példát majd a HITS-algoritmus.

\subsection{Vektorok és mátrixok normája}
Hogy a szinguláris felbontással kapcsolatos tételeket bizonyítani tudjuk, szükségünk lesz a mátrixnormák, fõként a \emph{Frobenius-norma} ismeretére.

\begin{subequations}
A \lekep[\nu]{\Rnm{n}{m}}{\mathbb{R}} leképezést \emph{mátrixnormának}
nevezzük, ha tetszõleges $A,B \in \Rnm{n}{m}$ mátrixszal és $\alpha \in \mathbb{R}$ számmal  kielégíti az alábbi három összefüggést:
\begin{align*}
A \neq 0      &\kov \nu(A) > 0 \\
\nu(\alpha A) &= |\alpha| \nu(A) \\
\nu(A+B)      &\leq \nu(A) + \nu(B) 
\end{align*}
A mátrixnorma $m=1$ speciális eseteként adódik a \emph{vektornorma}. A következõkben néhány szót ejtünk errõl, azt követõen pedig a mátrixnormákról.

\subsubsection{Vektornormák}

Vektorok \emph{2-normáját} az alábbi egyenlõséggel definiáljuk.
\begin{defin}[2-norma]
 $v\in \mathbb{R}^n$ 2-normája
\[\norma[2]{v} \defegy \sqrt{ \sum_{i=1}^n  |v_i|^2  } = \sqrt{v^{T} v}\,.\]
\end{defin}
Az elsõ két normaaxióma nyilvánvalóan teljesül, a háromszög-egyenlõtlenség pedig a Cauchy-egyenlõtlenség felhasználásával igazolható. A Cauchy-egyenlõtlenség vektorok skaláris szorzata és 2-normája közti kapcsolatot írja le:
\begin{tetel}[Cauchy-egyenlõtlenség] Ha $x,y \in \mathbb{R}^n$, akkor
\[|x^{T} y | \le \norma[2]{x} \norma[2]{y}, \]

ahol egyenlõség pontosan akkor van, ha $x$ és $y$ lineárisan összefüggõk.
\end{tetel}

Vektornormákat más módon is definiálhatnánk, például négyzetgyökvonás és négyzetre emelés helyett tekinthetnénk $1/p$-edik illetve $p$-edik hatványt is. Azonban a 2-normát egy fontos tulajdonság emeli ki a lehetséges vektornormák közül. 
\begin{defin}[Unitér-invariancia vektornormákra]
Egy vektornormát \emph{unitér-invariánsnak} nevezünk, ha tetszõleges $x$ vektor 
és $U$ ortogonális mátrix esetén 
\begin{align*}
\norma{x}&=\norma{Ux}\,.
\end{align*}
\end{defin}
Mielõtt megmutatjuk, hogy a 2-norma rendelkezik a fenti tulajdonsággal, megadjuk az ortogonális, vagy unitér mátrixok definícióját és egy fontos tulajdonságát.
\begin{defin}
Egy mátrixot ortogonálisnak, vagy unitérnek nevezzük, ha $U^TU=I$, azaz oszlopai egységhoszzúak és egymásra páronként merõlegesek.
\end{defin}

\begin{lemma}
Egy ortogonális mátrixra $UU^T=I$.
\end{lemma}

\begin{biz}
Mivel $U^TU=I$, $U^{-1}=U^T$. Az állítás adódik felhasználva, hogy a mátrix-szorzás esetében a bal és jobb inverz azonos.
\end{biz}

Most megmutatjuk, hogy a 2-norma unitér invariáns:
$$
 \norma[2]{x}^2=x^{T} x=x^{T} U^{T} U x =(Ux)^{T} (Ux)=\norma[2]{Ux}^2,
$$
tetszõleges $x$ vektorral és $U$ ortogonális mátrixszal, ahonnan adódik az állításunk helyessége. 
Másrészt a fordított irányhoz legyen {\normafv} egy tetszõleges unitér-invariáns norma, és $x_1$ egy 
olyan vektor, melyre $\norma{x_1}=1$. Ekkor az \halmaz{x}{\norma{x}=1} halmaz elemei felírhatóak 
$U x_1$ alakban valamely $U$ ortogonális mátrixszal. Az elõzõ egyenlõséglánc felhasználásával e halmaz 
elemeinek 2-normája megegyezik a $\norma[2]{x_1}$ mennyiséggel. Ezzel igazoltuk a következõ állítást:
\begin{allitas}\label{UnitInvVekt}
Egy {\normafv}  vektornorma pontosan akkor {\unitinv}, ha valamely $c \ge 0$ konstansra 
$\normafv = c \normafv[2]$, azaz ha a 2-norma konstans többszöröse.
\end{allitas}

\subsubsection{Mátrixok normái}
Most áttérünk mátrixok normáinak tárgyalására. Két különbözõ mátrixnormát fogunk bevezetni és használni. Elõször a vektorok 2-normájához hasonló módon definiálunk mátrixnormát, amelyet \emph{Frobenius-normának} nevezünk:
\begin{defin}[Frobenius-norma]
\begin{align*}
\norma[F]{A} &\defegy \sqrt{\sum_{i=1}^{n}  \sum_{j=1}^{m} A_{ij}^2 }\, ,
\end{align*}
\end{defin}

A Frobenius-norma egy a Cauchy-egyenlõtlenséghez hasonló összefüggést is kielégít.
\begin{allitas}\label{FrobKonz}
Tetszõleges $A \in \Rnm{n}{m}, B \in \Rnm{m}{p}$ mátrixok esetén
\begin{align*}
\norma[F]{AB} \leq \norma[F]{A} \norma[F]{B}.
\end{align*}
\end{allitas}
A bizonyítás elemi lépésekkel végezhetõ a Cauchy-egyenlõtlenség felhasználásával. Egy mátrixnormát \emph{konzisztensnek} nevezünk, ha összeszorozható mátrixok és szorzatuk kielégíti a fenti egyenlõtlenséget. Ezzel a tulajdonsággal nem minden mátrixnorma rendelkezik, pedig a konzisztenciára nagy szükség van iterációs algoritmusok hibabecslésénél.
%Ilyen eljárásokban az $n$-edik lépés hibavektorát az $n-1$-edikbõl egy iterációs mátrix szorzásával kapjuk. Ha például az összes iterációs mátrix normája kisebb $\frac{1}{2}$-nél, akkor a norma konzisztenciájából adódik a hiba $(\frac{1}{2})^n$ sebességû lecsengése. 

A második általunk használt mátrixnormát úgy definiáljuk, hogy az konzisztens legyen a vektorok 2-normájával, azaz szeretnénk, ha minden $x$ vektorra és $A$ mátrixra teljesülne a következõ egyenlõtlenség: 
\begin{align*}
\norma[2]{Ax} \le \norma{A} \norma[2]{x}.
\end{align*}
Az egyenlõtlenségbõl alsó becslés adódik \norma{A} értékére. Ezzel az alsó becsléssel definiáljuk mátrixok \emph{2-normáját}, más néven \emph{spektrálnormáját}.
\begin{defin}[Spektrálnorma]
\[\norma[2]{A} \defegy \max_{\norma[2]{x}=1} \norma[2]{Ax}\,.\]
\end{defin}
A képletet röviden úgy értelmezhetjük, hogy egy $A$ mátrix esetén \norma[2]{A} egy $x$ vektor $A$ mátrixszal történõ szorzásával elérhetõ \idez{legnagyobb nyújtás} mértékét határozza meg. A normaaxiómák teljesülése mindkét esetben egyszerûen ellenõrizhetõ a vektorok 2-normájának tulajdonságaiból.

Most néhány olyan összefüggést mutatunk be, melyek szintén meghatározzák az eddig megismert mátrixnormákat. Tetszõleges $A \in \Rnm{n}{m}$ mátrix esetén az $A^{T}A$ mátrix szimmetrikus és pozitív szemidefinit, ezért sajátértékei nemnegatív valós számok, melyeket $\lambda_m \ge \dots \ge \lambda_1 \ge 0 $ jelöl. 
\begin{allitas} \begin{eqnarray}
\norma[F]{A}      &=&  \sqrt{ \nyom{A^{T} A} } \label{normaAegyAHegyAllitas} \\
\norma[F]{A}      &=&  \sqrt{ \lambda_1+\lambda_2+\dots +\lambda_m }  \label{FNormaSajatErt}\\
\norma[2]{A}      &=&  \sqrt{\lambda_m} \label{2NormaSajatErt}\\ 
\norma[2]{A}      &=&  \max_{\norma[2]{x}=1,\, \norma[2]{y}=1} |y^{T} A x| \label{MaxyAx}  \\
\norma[2]{A}      &=&  \norma[2]{A^{T}} \label{normaAegyAHegyAT} 
\end{eqnarray}
\end{allitas} 
\begin{biz}
Az $A^{T}A$ mátrix fõátlójában szereplõ $j$-edik elem megegyezik az $A$ mátrix $j$-edik oszlopában elõforduló elemek négyzetösszegével. Így $\nyom{A^{T} A}$ nem más mint $A$ összes elemének négyzetösszege. Ezzel igazoltuk az (\ref{normaAegyAHegyAllitas}) egyenlõséget. 

Az (\ref{FNormaSajatErt}) összefüggés pedig (\ref{normaAegyAHegyAllitas}) következménye, hiszen egy mátrix nyoma megegyezik sajátértékeinek összegével.

Az (\ref{2NormaSajatErt}) összefüggést a következõ módon kapjuk meg:
$$
\norma[2]{A}
=\max_{\norma[2]{x}=1} \norma[2]{Ax}
=\max_{\norma[2]{x}=1} \sqrt{|x^{T}A^{T}Ax| }
=\sqrt{\lambda_1} \,.
$$
A (\ref{MaxyAx}) egyenlõség esetén a $\ge$ irány a Cauchy-egyenlõtlenségbõl adódik, mert 
$$\norma[2]{Ax}=\norma[2]{y} \norma[2]{Ax} \ge |y^{T} A x |\, ,$$
tetszõleges $y$ és $x$ vektorra, melynél $\norma[2]{x}=\norma[2]{y}=1$.
 Legyen $x_1$ az $\norma[2]{A x}$ maximumát beállító vektor, ezt behelyettesítve az elõzõ 
egyenlõtlenségbe kapjuk a $\ge$ irányt. Másrészt $x_1$ és
$y_1 \legy \frac{1}{\norma[2]{A x_1}}A x_1 $ választással
 $$|y_1^T A x_1|=\norma[2]{Ax_1}=\norma[2]{A}$$
adódik, amivel a $\le$ irányt is beláttuk. 
Végül az (\ref{normaAegyAHegyAT}) összefüggés (\ref{MaxyAx}) következménye,
 hiszen a kifejezésben egy skalár szerepel, amelynek transzponáltja önmaga, $|x^TA^Ty|$, azaz az egyenlõségben $A$ lecserélhetõ $A^T$-ra.
\end{biz}
%
Mátrixok normájához is társítható a vektorokéhoz hasonló geometriai jelentés.
Tetszõleges $A$ mátrix esetén az 
$$ 
E_A \defegy \halmaz{Ax}{\norma[2]{x}=1} 
$$ 
egy ellipszoidot határoz meg, melynek dimenziója $\rang{A}$. A mátrix normája szemléletesen ennek az  ellipszoidnak a \idez{nagyságát} jelenti.
Az ellipszoidot körbevehetjük egy olyan $\rang{A}$-dimenziós téglatesttel, melynek élei párhuzamosak a fõtengelyekkel, és az élek hossza megegyezik a fõtengelyek hosszával. Ekkor a Frobenius-norma megegyezik a téglatest leghosszabb testátlójával, a 2-norma pedig annak leghosszabb élével, vagyis a legnagyobb fõtengely hosszával. Ez (\ref{FNormaSajatErt}) és az (\ref{2NormaSajatErt}) egyenlõségbõl adódik, felhasználva, 
hogy az ellipszoid fõtengelyeinek hossza megegyezik $A^{T} A$ sajátértékeivel \cite{Rozsa}. 

Ha egy $A$ mátrixot az $U^T$ és a $V$ ortogonális mátrixokkal szorzunk,  akkor $E_A$-ból forgatással nyerjük az $E_{U^{T} A V}$ ellipszoidot. A mátrix normájától azt várnánk, hogy ne változzon e mûvelet hatására. E tulajdonság azonban nem igaz tetszõleges normára.
\begin{defin}[Unitér-invariania mátrixnormákra]
Egy {\normafv} mátrixnormát \emph{{\unitinv}nak} mondunk, ha tetszõleges $U$ és $V$ ortogonális mátrixokkal
$$ \norma{U^{T} A V} = \norma{A}. $$
\end{defin}
\begin{allitas}
A Frobeinus- és a 2-normák {\unitinv}ak.
\end{allitas}
\begin{biz}
A 2-normára már korábban beláttuk, a Frobenius-normára pedig nyilvánvaló, hogy egy mátrix és 
transzponáltja esetén megegyezik, ezért elég azt megmutatni, hogy egy ortogonális mátrixszal balról szorozva nem változik a norma. Utóbbi a 2-norma esetén abból adódik, hogy a maximalizálandó $\norma[2]{Ax}= \norma[2]{U^{T} Ax} $ bármely $x$ esetén. A Frobenius-norma {\unitinvanci}ája szintén visszavezethetõ vektorok 2-normájáéra, hiszen az $U$ mátrixszal szorozhatunk oszloponként, és a normát is számíthatjuk az oszlopokéból.   
\end{biz}
A dolgozat hátralevõ részében vektorok 2-normájára egyszerûen a \normafv jelölést, mátrixokéra pedig a \normafv[2] jelölést, Frobenius-normára  a \normafv[F] jelölést használjuk. 

\end{subequations}
\subsection{Szinguláris felbontás létezése}
Most már minden eszközünk megvan, hogy a szinguláris felbontást definiáljuk és létezését bizonyítsuk.
\begin{defin}[Szinguláris felbontás]
Egy $A \in \mathbb{R}^{n\times m}$ mátrix szinguláris felbontásán az olyan
\[A = U \Sigma V^T\]
szorzattá bontást értjük, ahol $U \in \mathbb{R}^{n\times n}$ és
$V \in \mathbb{R}^{m\times m}$ ortogonális mátrixok.  A
$\Sigma\in\mathbb{R}^{n\times m} mátrix pedig diagonális$, és a ,,fõátlójában'' a
$\sigma_1\geq\cdots\geq\sigma_r>0$ pozitív számokat csupa $0$ követi.
\end{defin}
\begin{defin}[Szinguláris értékek]
A $\sigma_i$ értékeket az $A$ mátrix szinguláris értékeinek hívjuk.
\end{defin}
\begin{defin}[Szinguláris vektorok]
A $U$ és $V$ oszlopait az $A$ mátrix bal, illetve jobboldali szinguláris vektorainak hívjuk.
\end{defin}
\begin{tet}[SVD létezése]
Tetszõleges $A \in \mathbb{R}^{n \times m}$ $r$ rangú mátrixnak létezik szinguláris felbontása.
\end{tet}
\begin{biz}
Teljes indukció $min\left(m,n\right)$-re.
Ha $min\left(m,n\right)=1$, akkor az $A$ mátrix egyedül az $a := A$ vektorból áll. Feltehetjük, hogy $a$ oszlopvektor. Az $a$-hoz található olyan ortogonális $U$ mátrix, melyre $U^Ta=||a||e_1$. Így $\Sigma = ||a||e_1$ és $V=(1)$ választással adódik a $min\left(m,n\right)=1$ eset.

Ha $min\left(m,n\right) > 1$, akkor a problémát visszavezetjük egy $(n-1)\times(m-1)$ méretû mátrix felbontására. Feltehetõ, hogy $A \neq 0$, különben triviális a felbontás.
\begin{align*}
\sigma_1 &\legy \norma[2]{A}=  \max_{\norma{v}=1} \norma{Av} &
v_1      &\legy \argmax_{\norma{v}=1} \norma{Av} &
u_1      &\legy \frac1\sigma_1 A v_1
\end{align*}
Egészítsük ki továbbá az $u_1$ és $v_1$ vektorokat 
$U_1 \legy (u_1 \, u_2 \, \dots \, u_n)$  és 
$V_1 \legy (v_1 \, v_2 \, \dots \, v_m)$ ortogonális mátrixokká. Ekkor a 
$w ,\, z$ és $A_2$ jelöléseket bevezetve
\begin{align*}
U_1^{T} A V_1 &= \left( \begin{array}{cc}
 \sigma_1 & w^{T} \\
  z       &  A_2
 \end{array} \right)
,&
U_1^{T} A v_1 &= \left( \begin{array}{c}
 \sigma_1  \\
  z       
 \end{array} \right)
,& 
V_1^{T} A^{T} u_1 &= \left( \begin{array}{c}
 \sigma_1 \\
  w
 \end{array} \right).
\end{align*}
Az (\ref{normaAegyAHegyAllitas}) állítás szerint 
$\sigma_1=\norma[2]A=\norma[2]{A^{T}}$,  ezért az $\norma{u_1}=\norma{v_1}=1$
 vektorok esetén $\norma{Au_1} \le \sigma_1$ és 
$\norma{A^{T} v_1} \le \sigma_1$. 
Innen a  vektornorma {\unitinvanci}áját felhasználva kapjuk, hogy 
\begin{align*}
\norma{  \left( \begin{array}{c}
 \sigma_1  \\
  z       
 \end{array} \right)}&\le \sigma_1 & 
 &\text{és}&
\norma{  \left( \begin{array}{c}
 \sigma_1  \\
  w      
 \end{array} \right)} &\le  \sigma_1 .       
\end{align*} 
Ez viszont csak úgy lehetséges, ha $z=w=0$, vagyis igazoltuk, hogy 
$$
U_1^{T} A V_1 = \left( \begin{array}{cc}
 \sigma_1 & 0 \\
  0      &  A_2
 \end{array} \right).
$$
Indukciós feltevésünk alapján az $(n-1) \times (m-1)$ méretû $A_2$ mátrixhoz 
találhatók $U_2$ és $V_2$ ortogonális mátrixok, melyekkel $U_2^{T} A_2 V_2 = \Sigma_2$. Az 
\begin{align*}
U^{T} &\legy
\left( \begin{array}{cc}
  1      &  0 \\
  0      &  U_2^{T}
 \end{array} \right) U_1^{T} &
 &\text{és}&
V^{T} &\legy
V_1 \left( \begin{array}{cc}
  1      &  0 \\
  0      &  V_2
 \end{array} \right) 
\end{align*}
választással kapjuk a kívánt felbontását $A$-nak:
$$ 
U^{T} A V= 
\left( \begin{array}{cc}
  \sigma_1      &  0 \\
  0      &  \Sigma_2
 \end{array} \right) 
=\Sigma \,.
$$
\end{biz}

\subsection{HITS-algoritmus}
Jon Kleinberg\cite{kleinberg99authoritative} 1998-ban született HITS-algoritmusa \emph{(Hyperlink-Induced Topic Search)} az internetes keresések
találatait rangsorolja. 
A rangsort az oldalak közt lev? hiperlinkek alapján állítja fel.

Az algoritmus ötlete, hogy két fontos csoportba osztja az oldalakat, központi \emph{(hub)} és mértékadó \emph{(authority)} oldalakra.
Az algoritmus iteratív módon minden oldalra frissíti a központiság és mértékadóság mértékét.
Kleinberg meghatározásában egy oldal annál központibb, minél mértékadóbb oldalakra mutat, és annál mértékadóbb, minél központibb oldalak mutatnak rá.

A feladat megfogalmazható az irányított gráfok nyelvén is. A weboldalak a csúcsok, az éleket pedig a hiperlinkek határozzák meg. Az $auth,hub:V\rightarrow\mathbb{R}$ függvényeket használjuk majd az egyes oldalak mértékadóságának és központiságának jelzésére.

\subsubsection{Az algoritmus}
A HITS heurisztikája szerint egy oldal központiságát az határozza meg, hogy mennyire mértékadó oldalakra mutat rá.
Tehát a központiságot a ki-élek határozzák meg.

Feltevésünk szerint egy oldal akkor lesz mértékadóbb, ha a központibb oldalak mutatnak rá. Így a mértékadóság a be-élekbõl fog származni.

Az algoritmus a következõképpen mûködik.

\bigskip
\textbf{HITS-algoritmus}

\begin{enumerate}
\item Tetszõleges $auth$, és $hub$ kezdõértéket választunk.
\item Frissítjük minden csúcs $auth$ értékekét a beélek mentén a szomszédok aktuális $hub$ értéke szerint.\label{iterLepes}
\[auth^{\left(k+1\right)}\left(v\right)=\sum_{\left(w,v\right)\in E}hub^{\left(k\right)}\left(w\right)\]
\item Frissítjük minden csúcs $hub$ értékét a kiélek mentén a 
szomszédok aktuális $auth$ értéke szerint.
\[hub^{\left(k+1\right)}\left(v\right)=\sum_{\left(v,w\right)\in E}auth^{\left(k+1\right)}\left(w\right)\]
\item Normáljuk az $auth$ és $hub$ értékeket.
\item Ellenõrizzük, hogy az értékek változása a bemenetként megadott küszöb alatt van-e.  Ha igen, megállunk.
\item Visszatérünk a \ref{iterLepes}. pontra
\end{enumerate}

A normalizációs lépés segítségével elkerülhetjük a numerikus túlcsordulást. A normálás, mint konstanssal való szorzás, a központiság és mértékadóság sorrendjén nem változtat, tehát a leállási feltétellel összhangban teszõleges módon végrehajtható. Látni fogjuk, hogy az $auth$ és $hub$ értékek normálás nélkül általában végtelenhez tartanak.

\subsubsection{Az algoritmus elemzése}
%Az algoritmussal kapcsolatban számos kérdés merül fel.
%\begin{itemize}
%\item Konvergál-e az algoritmus?
%\item Milyen értékeket érdemes $auth^{\left(1\right)}$-nak és $hub^{\left(1\right)}$-nak választani?
%\item Hogyan lehet az $auth$ és $hub$ értékeket hatékonyan számolni?
%\end{itemize}

%A kérdések megválaszolását segíti, ha
Az algoritmus egy lépését az adjacenciamátrix segítségével írjuk fel, majd használjuk a szinguláris felbontást.

Készítsünk az $auth$ és $hub$ értékekbõl egy $a$ és $h$ sorvektort!
\[a^{(k)} = (auth^{(k)}(v_1),\dots , auth^{(k)}(v_n))\]
\[h^{(k)} = (hub^{(k)}(v_1),\dots , hub^{(k)}(v_n))\]

Az így definiált vektorokkal és az $A$ adjacenciamátrixszal a következõképp írhatjuk fel az iterációs lépést:
\[a^{\left(k+1\right)}= h^{\left(k\right)}A\]
\[h^{\left(k+1\right)}= a^{\left(k+1\right)}A^T.\]
A két egyenletet egymásba helyettesítve az alábbiakat kapjuk.
\[a^{\left(k+1\right)}=a^{\left(k\right)}A^TA=a^{\left(1\right)}\left(A^TA\right )^k\]
\[h^{\left(k+1\right)}=h^{\left(k\right)}AA^T=h^{\left(1\right)}\left(AA^T\right )^k\]
Alkalmazzuk $A=U\Sigma V^T$ szinguláris felbontását. Mivel $A$ négyzetes mátrix, ezért $\Sigma$ is az, így $\Sigma=\Sigma^T$.
\[a^{\left(k+1\right)}=a^{\left(1\right)}\left(A^TA\right )^k=
a^{\left(1\right)}V\Sigma U^TU\Sigma V^TV\Sigma\dots U\Sigma V^T=
a^{\left(1\right)}V\Sigma^{2k}V^T\]
\begin{equation}
h^{\left(k+1\right)}=h^{\left(1\right)}\left(AA^T\right )^k=
h^{\left(1\right)}U\Sigma V^TV\Sigma U^TU\Sigma\dots V\Sigma U^T=
h^{\left(1\right)}U\Sigma^{2k}U^T \label{hitsEgyenlet}
\end{equation}

Most már jól látható, hogy a mértékadóságot és a központiságot a szinguláris értékek és vektorok határozzák meg.
Normalizáljuk a $\Sigma$ mátrixot $\sigma_1$-gyel.
\[\Sigma_1 = \left( \begin{array}{cccc}
\frac{\sigma_1}{\sigma_1} & 0 & \dots & 0 \\ 
0 & \frac{\sigma_2}{\sigma_1} & & \vdots\\ 
\vdots & &\ddots & 0  \\ 
0 & \dots & 0 & \frac{\sigma_n}{\sigma_1} \\ 
\end{array}
\right), \quad
\Sigma_1^{2k} = \left( \begin{array}{cccc}
1 & 0 & \dots & 0 \\ 
0 & (\frac{\sigma_2}{\sigma_1})^{2k} & & \vdots\\ 
\vdots & &\ddots & 0  \\ 
0 & \dots & 0 & (\frac{\sigma_n}{\sigma_1})^{2k} \\ 
\end{array}
\right)
\]
Tehát az elsõ elemet kivéve a mátrix összes többi eleme az iterációszámban exponenciális sebességgel tart $0$-hoz.

Ha $h^{(1)}$-t úgy választjuk, hogy merõleges legyen $U$ elsõ oszlopára, akkor ezzel eliminálhatjuk az elsõ szinguláris vektor hatását. Ilyenkor a második lesz a hangsúlyos szinguláris vektor.

Tehát a HITS-algoritmus alkalmas a szinguláris felbontás meghatározására.
Ha már ismert az elsõ $k$ szinguláris vektor, akkor választhatunk rájuk merõleges kiinduló vektort. Ekkor a HITS a $k+1$ szinguláris vektorhoz fog konvergálni.

Szemléletes jelentését is társíthatunk az $A^k$, valamint az $(A^TA)^k$ mátrixokhoz.
$a_{ij}$-re felfoghatjuk úgy is, mint az $i$ és $j$ csúcsok közötti $1$ hosszú séták számát. Most nézzük $A^2$-t!
\[A^2_{ij} = \sum_{k=1}^n a_{ik}a_{kj}\]
Látható, hogy $A^2_{ij}$ az $i$ és $j$ közötti $2$, hosszú séták számával lesz egyenlõ.
Indukcióval bizonyítható, hogy ez magasabb hatványokra is igaz.

$(AA^T)^k$ esetében hasonló eredményre jutunk, de most minden második lépésünk a megfordított gráfon történik. Látható tehát, hogy az $auth$ és $hub$ értékek is az adott csúcsból induló és érkezõ oda-vissza utak számával arányosak.

Vizsgáljuk meg $A^TA$ hatását a $v{(i)}$ szinguláris vektorokra!
$A^TAv_{(i)}$-t 
\[A^TAv_i=V\Sigma^TU^TU\Sigma V^Tv_i = V\Sigma^2V^Tv_i=V\Sigma^2\left(\begin{array}{c}
0 \\
\vdots \\
1 \\
\vdots \\
0
\end{array}
\right) = 
V\left(\begin{array}{c}
0 \\
\vdots \\
\sigma_i \\
\vdots \\
0
\end{array}
\right) = \sigma_i v_i
\]
Látható, hogy a szinguláris értékek az $A^TA$ sajátértékei.
A jobb-szinguláris vektorok a sajátvektorok.

\section{A spektrum}
%TODO Kell ide több szöveg?
Az elõzõ fejezetben láttuk, hogy egy gráf adjacenciamátrixának sajátértékei és sajátvektorai különleges jelentéssel bírnak, és információt hordoznak a gráf struktúrájáról.

Ebben a fejezetben áttekintjük a gráfok spektrálgráfelméletben leggyakrabban használt mátrixreprezentánsait. Bevezetjük a spektrum fogalmát, és megvizsgáljuk
annak alapvetõ tulajdonságait.

Majd a fejezet végén megadjuk néhány konkrét gráfcsalád spektrumát.

\subsection{A Laplace-mátrix és a normalizált Laplace-mátrix}
%Egyszerû gráfokra mutatunk ilyen kedvezõbb mátrixot, az ún. Laplace-mátrixot.
%Legyen $G=(V,E)$ egy egyszerû gráf a $|V|=n$ csúcshalmazon, az $|E| \leq \binom{n}{2}$
%élhalmazzal! Jelölje $d_v$ a $v$ csúcs fokszámát. Tekintsük a következõ $L$ mátrixot:
%\begin{defin}[Laplace-mátrix egyszerû gráfokra]\cite{chung1997spectral}
%\[L(u,v)=\left\{
%\begin{array}{cc}
%d_v & \textrm{ha $u = v$ }\\ 
%-1 & \textrm{ha $u$ és $v$ szomszédos} \\ 
%0 & \textrm {egyebkent}
%\end{array} \right.\]
%\end{defin}
A dolgozat hátralevõ részében legtöbbször hasonlósággráfokról mondunk ki állításokat. Ezek olyan egyszerû gráfok, melyekhez adva van egy $w:V\times V \rightarrow \mathbb{R}$ súlyfüggvény, amelyre igazak az alábbiak.
\[w(u,v)=w(v,u)\]
\[w(u,v) \geq 0\]
\[uv \textrm{ nem él} \Leftrightarrow w(u,v)=0\]
A $w(u,v)$ értékeket mátrixba rendezve kapjuk a $W$ mátrixot.
Vegyük észre, hogy a $W$ önmagában meghatározza a gráfot, így a hasonlósággráfokat a $V$ csúcshalmazzal és a $W\geq 0$ súlymátrixszal fogjuk megadni.

A hasonlósággráfok esetében definiálhatunk egy általánosabb fokszámfogalmat.
\begin{defin}[Általánosított fokszám]
$G=(V,W)$ hasonlósággráf egy $v$ csúcsának általánosított fokszáma
\[d_v=\sum_{w(u,v) > 0} w(u,v).\]
\end{defin}
Most már definiálhatjuk egy hasonlósággráf Laplace-mátrixát.
\begin{defin}[Laplace-mátrix]
Egy $G=(V,W)$ hasonlósággráf Laplace-mátrixa
\[L(u,v)=\left\{
\begin{array}{cc}
d_v-w(u,v) & \textrm{ha $u = v$ }\\ 
-w(u,v) & \textrm{ha $u$ és $v$ szomszédos} \\ 
0 & \textrm {egyebkent}
\end{array} \right.\]
\end{defin}
Jelölje $D\in\mathbb{R}^{n\times n}$ a fokszámmátrixot, vagyis azt a mátrixot, amiben minden fõátlón kívüli elem $0$, a fõátlóban pedig rendre $d_v$ áll.
Ezt a jelölést használva a Laplace-mátrixot a következõ formában is felírhatjuk:
\[L=D-W.\]
A Laplace-mátrix sok elõnyös tulajdonsággal rendelkezik. Szimmetrikus és pozitív-szemidefinit, emiatt a sajátértékei nemnegatív, valósak.
Hogy a sajátértékeket korlátok közt tartsuk, érdemes a Laplace-mátrixot az általánosított fokszámokkal súlyozni. Így kapjuk a normalizált Laplace-mátrixot.
\begin{defin}[Normalizált Laplace-mátrix]
Egy $\wg$ gráf normalizált Laplace mátrixa
\[\mathcal{L}=D^{-\frac{1}{2}}LD^{-\frac{1}{2}} = I-D^{-\frac{1}{2}}WD^{-\frac{1}{2}}.\]
\end{defin}
\begin{defin}[Spektrum] A $G=(V,W)$ gráfhoz tartozó $\mathcal{L}$ normalizált Laplace-mátrix sajátértékeit
\[0=\lambda_0\leq\lambda_1\leq\lambda_2\dots\leq\lambda_{n-1}\]
a $G$ spektrumának hívjuk.
\end{defin}

A definícióból látható, hogy a spektrum nem függ a csúcsok sorrendjétõl, tehát két izomorf gráf spektruma megegyezik.
Fordítva azonban ez nem igaz, két azonos spektrumú gráf nem feltétlenül lesz izomorf.

\subsection{A spektrum néhány egyszerû tulajdonsága}
Mivel $\mathcal{L}$ szimmetrikus, pozitív-szemidefinit, ezért az összes sajátértéke valós nemnegatív.

Vegyünk egy tetszõleges $g:V\rightarrow\mathbb{R}$ függvényt. És nézzük $g$-t, mint oszlopvektort. Ekkor tekinthetjük az ún. \textit{Rayleigh-hányadost}
\begin{eqnarray}
%\frac{g^T\mathcal{L}g}{\langle g,g\rangle} =
\frac{\langle g, \mathcal{L}g\rangle}
{\langle g,g\rangle}
=
\frac{\langle g,D^{-\frac{1}{2}}LD^{-\frac{1}{2}}g \rangle}
{\langle g,g \rangle}
=\frac{\langle
f,Lf \rangle}{
\langle
D^{\frac{1}{2}}f,D^{\frac{1}{2}}f 
\rangle
}
=
\frac{\sum_{u \sim v} \left(f\left(u\right)-f\left(v\right)\right)^2w(u,v)}
{\sum_v f\left(v\right)^2 d_v}, \label{Rayleigh}
\end{eqnarray}
ahol $g=D^\frac{1}{2}f$ és $u\sim v$ jelöli azokat a rendezetlen
csúcspárokat, amikre $u$ és $v$ szomszédosak, valamint $\langle ,\rangle$ jelöli az
$\mathbb{R}^n$
-beli belsõszorzatot.

\eqref{Rayleigh}-ból is látszik, hogy az összes sajátérték nemnegatív és a $0$ is sajátérték. A $0$-hoz tartozó sajátvektor a
$\sqrt{\underline{d}}=\left(\sqrt{d_1},\dots,\sqrt{d_n}\right)^T$ vektor.
A $0$ akkor és csak akkor egyszeres sajátérték, ha a gráf összefüggõ.
Ha a gráf nem összefüggõ, akkor a spektrum az egyes összefüggõ részek spektrumának uniója.
Ezt abból is láthatjuk, hogy ilyenkor az adjacenciamátrix blokkosított alakra hozható. Ha
$g$ $\mathcal{L}$
sajátvektora, akkor a Rayleigh-hányados lesz a hozzá tartozó sajátérték.

Továbbá,
\begin{equation}
\lambda_1 = \inf_{f_\perp D\textbf{1}}
\frac{\sum_{u \sim v} \left(f\left(u\right)-f\left(v\right)\right)^2w(u,v)}
{\sum_v f\left(v\right)^2 d_v} = \min_{\norma{g}=1\atop{g^T\sqrt{\underline{d}} = 0}}
g^T\mathcal{L}g\label{minLemma}
\end{equation}

és
\begin{equation}
\lambda_{n-1}=\max_{\norma{g}=1}g^T\mathcal{L}g. \label{maxLemma}
\end{equation}

Az elõzõ $min$ és $max$ egyenlõség hasonlóképp igaz a Laplace-mátrixra és annak sajátértékeire. Érdemes az imént vizsgált kifejezést magasabb dimenzióra is kiterjeszteni. 

$X\in \mathbb{R}^{n\times k}$ adott szubortogonális mátrix. $X$ oszlopait az $x_1,\dots,x_k$ vektorokkal, sorai pedig az $r^T_1,\dots,r^T_n$ vektorokkal jelöljük.
\[
Q_k = tr(X^TLX)= \sum_{l=1}^k x_l^T (D-W) x_l =
\sum_{i=1}^n d_i \norma{r_i}^2 - \sum_{i=1}^n \sum_{j=1}^n w_{ij}r_i^Tr_js = \] 
\\
\begin{equation}
\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n w_{ij} \norma{r_i-r_j}^2 \label{Qk}
\end{equation}
Vegyük észre, hogy a $k=1$ esetben a Rayleigh-hányadost kapjuk vissza.

Tudjuk, hogy egy mátrix nyoma megegyezik a sajátértékeinek összegével. Emiatt
\[\sum_{i=0}^n \lambda_i = tr\left(\mathcal{L}\right) = n .\]
Így ki tudjuk számolni a spektrum átlagát,
amivel becsülni tudjuk a legnagyobb és legkisebb pozitív sajátértéket.
\begin{equation}
\lambda_1 \leq
\frac{\sum_{i=1}^{n-1}\lambda_i}{n-1} = \frac{n}{n-1} \leq
\lambda_{n-1}\label{eq:lambdaAtlag}
\end{equation}

Látni fogjuk majd, hogy teljes gráfra egyenlõséggel teljesül mindkét egyenlõtlenség.
Nem teljes gráfra azonban már az is igaz, hogy $\lambda_1 \leq 1$.\quad \cite{chung1997spectral}

\subsection{Néhány speciális gráf spektruma}
$\mathbf{K_n}.$
Az $n$ csúcsú teljes gráf spektruma szimmetria okokból és mert $tr \mathcal{L} = n$:
\[\lambda_0 = 0, \lambda_1 = \dots = \lambda_{n-1} = \frac{n}{n-1} . \quad \cite{bolla}\] 
Ebbõl látható, hogy a \eqref{eq:lambdaAtlag} becslés éles.

$\mathbf{K_{m,n}}.$
Az $m$ és $n$ elemû osztályokból álló teljes páros gráf spektruma:
\[\lambda_0 = 0, \lambda_1 = \dots = \lambda_{n+m-2} =1, \lambda_{n+m-1} = 2.\quad\cite{bolla}\]

$\mathbf{S_n}.$
Az $n$ ágú, azaz $n+1$ csúcsú csillag spektruma, mivel $S_n = K_{n,1}$:
\[\lambda_0 = 0, \lambda_1 = \dots = \lambda_{n-1} =1, \lambda_{n} = 2 .\]

$\mathbf{P_n}.$
Az $n$ hosszú út spektruma:
\[\lambda_k= 1-\cos\frac{\pi k}{n-1} \quad k=0,\dots,n-1.\quad\cite{cvetkovic1980spectra}\]

$\mathbf{C_n}.$
Az $n$ hosszú kör spektruma:
\[\lambda_k=1-\cos\frac{2\pi k}{n} \quad k=0,\dots,n-1.\quad\cite{chung1997spectral}\] 

$\mathbf{Q_n}.$
A $2^n$ csúcsú $n$ dimenziós hiperkocka sajátértékei:
\[\frac{2k}{n}\quad\binom{n}{k} \textrm{ multiplicitással, } k=0,\dots,n.\quad\cite{lovasz2007combinatorical}\]

\section{Vágások és partíciók}
Gráfok vizsgálatának egyik fontos kérdése, hogy hogyan tudjuk egy gráf csúcsait jól csoportosítani, particionálni.
A csoportosítás által egyszerûsödik, jobban átlátható lesz a gráf szerkezete.
Elõkerülhetnek rejtett kapcsolatok, ami a gráf struktúrájának jobb megértéséhez vezet.

A gráfok klaszterei olyan csúcshalmazok, amelyek valamilyen
jellemzõ tulajdonság alapján mérhetõen elkülönülnek.
Ilyenre példa a szociális hálókban az azonos érdeklõdésû emberek csoportja, a fehérjehálózatokban az egymással reagáló vegyületek vagy képek esetében az alakzathoz tartozó pixelek.

Particionálás során minden csúcsot be kell sorolnunk egy klaszterbe.
Tehát a klaszterezés egy $k$-részre vágás, vagyis a $G=(V,W)$ gráf csúcsainak egy olyan $P_k=(V_1,\dots,V_k)$ felosztása, ahol $V_i$-k diszjunktak és \[\bigcup_{i=1}^k V_i =V\]. $k=2$ esetén biparticionálásról beszélünk. 

A megfelelõ particionálás matematikailag nehezen megfogható fogalom. Az általunk használt mértékekhez sok esetben könnyen találunk olyan példát, amikor az optimális particionálás messze van az elvárttól. Egy jó particionálástól elvárjuk, hogy az egy csoportban levõ elemek hasonlítsanak egymásra, míg a különbözõ csoportokban levõk különbözzenek. Valamint elvárt az is, hogy a klaszterek hasonló méretûek legyenek.

Az alábbiakban bevezetjük a vágás fogalmát, és bemutatunk különbözõ mértékeket vágásokra és partíciókra. Ezt követõen megvizsgáljuk a bipartíciók és multipartíciók kapcsolatát a spektrummal.

\subsection{Vágások}
Intuíció alapján két csúcshalmazt különbözõnek tekintünk, ha könnyen el tudjuk választani õket egymástól. Érdemes tehát megvizsgálni, hogy hány élt kell elhagyni ahhoz, hogy  kettéválasszuk õket. Az ilyen élhalmazokat vágásoknak hívjuk.

\begin{defin}[Vágás]
Egy gráf vágása az élek olyan részhalmaza, amely elhagyása után a gráf már nem összefüggõ.
\end{defin}
$\mathcal{C}(G)$-vel jelöljük a $G$ gráf vágásainak halmazát.
Ahhoz, hogy a vágásokat össze tudjunk hasonlítani, bevezetjük a vágás értékét.
\begin{defin}[Vágás értéke]
Egy $G=(V,W)$ gráf $C\subset E(G)$ vágásának értéke:
\[w(C)=\sum_{(u,v)\in C}w(u,v),\]
ahol $(u,v)$ rendezetlen párokat jelöl.
\end{defin}

Egy bipartíció rögtön meghatároz egy vágást is.
\[ \partial S = \big\{ \{ u,v \} \in E(G): u \in S , v \notin S \big\} \]
Világos, hogy $\partial S = \partial \overline{S}$.
%A vágás éleinek nem $S$-ben levõ végpontjait $\delta S$-el jelöljük majd.
%\[\delta S = \{v \notin S : \{u, v\} \in E(G), u \in S\}\]

Tetszõleges két csúcshalmazhoz is meghatározhatjuk az általuk feszített részgráfban az elválasztásukhoz szükséges vágás értékét.
\begin{defin}[Vágás értéke]\cite{bolla}
A $G=(V,W)$ élsúlyozott gráfhoz tartozó $S,T \in V$ nem üres csúcshalmazok által meghatározott vágás súlya
\[w(S,T)=\sum_{s\in S}\sum_{t\in T}w_{st}\].
\end{defin}
Ezzel a definíciót kiterjeszthetjük tetszõleges számú halmazra.
\begin{defin}[k-vágás értéke]
\[cut(P_k,G) = sum_{a=1}^{k-1}\sum_{b=a+1}^{k}w(V_a,V_b)\]
\end{defin}

Láttuk, hogy a spektrum erõs összefüggésben van a gráfok szerkezetével, így nem meglepõ, hogy a minimális vágás költsége sem független a spektrumtól. Felhívnánk a figyelmet, hogy a következõ állításban a sima Laplace-mátrix spektrumát használjuk fel. 
\begin{allitas} Az $n$ csúcsú $\wg$ összefüggõ gráf $L$ Laplace-mátrixának sajátértékeit jelölje 
$0=\lambda_0<\lambda_1\leq \dots \leq \lambda_{n-1}$. Ekkor 
\[\min_{C\in\mathcal{C}(G)}w(C) \geq \frac{n-1}{n	}\lambda_1.\]
\end{allitas}

\begin{biz}

Legyen $U^*$ egy min-vágás. Az $U$ segítségével definiáljuk az alábbi egydimenziós reprezentánsokat.
\[r_i= \left\{ \begin{array}{cc}
n-|U^*| & \textrm{ha } i \in U^* \\
-|U^*| & \textrm{ha } i \in \overline{U^*}
\end{array}
\right.
\]
Legyen $\textbf{r}$ az $r_i$ értékeket összefogó vektor. Normáljuk $\textbf{r}$-t.
\[\norma{\textbf{r}} = \sum_{i=1}^n r_i^2 =|U^*|(n-|U^*|)^2 +
(n-|U^*|)|U^*|^2 = |U^*|(n-|U^*|)n
\]
\[\tilde{\textbf{r}} = \frac{\textbf{r}}{\sqrt{|U^*|(n-|U^*|)n}}\]
Az \eqref{maxLemma} és \eqref{Qk}
alapján:
\[\lambda_{1}\leq \tilde{\textbf{r}}^TL\tilde{\textbf{r}} = \sum_{i<j}w_{ij}(\tilde{r}_i-\tilde{r}_j)^2 =
w(U^*,\overline{U^*})\frac{(n-|U^*|+|U^*|)^2}{|U^*|(n-|U^*|)n}.\]
Ebbõl következik, hogy
\[\min_{C\in\mathcal{C}(G)}w(C) \geq \frac{|U^*|(n-|U^*|)}{n}\lambda_{1}\geq\frac{n-1}{n}\lambda_{1}.\quad\cite{bolla}\]
\end{biz}

Az elõzõ bizonyítás menetét követve könnyen adódik az alábbi állítás.
\begin{allitas}[Max-vágás súlya]
Legyenek a $0=\lambda_0\leq\lambda_1\dots\leq\lambda_{n-1}$ a $G=(V,W)$ gráfhoz tartozó $L$ Laplace-mátrix sajátértékei. Ekkor
\[ \max_{U\in V}w(U,\bar{U}) \leq \frac{n}{4}\lambda_{n-1}.\]
\end{allitas}
\begin{biz}
Az elõzõ állítás bizonyítását csak annyiban módosítjuk, hogy a \eqref{maxLemma}-t használjuk, valamint, hogy nem minimális, hanem maximális vágást határozzon meg $U^*$.
Így kapjuk végül a kívánt egyenlõtlenséget.
\[\max_{C\in\mathcal{C}(G)}w(C) \leq \frac{|U^*|(n-|U^*|)}{n}\lambda_{n-1}\leq\frac{n}{4}\lambda_{n-1}\quad\cite{bolla}\]
\end{biz}

Fiedler\cite{fiedler1973algebraic} egy egyszerû gráfra a min. vágás értékét élösszefüggõségnek hívta, mivel ennyi élt kell eltávolítani ahhoz, hogy a gráf ne legyen összefüggõ. Az élösszefüggõségre az $e(G)$ jelölést vezette be. Mivel a $\lambda_1$ sajátérték és az élösszefüggés között szoros kétoldali kapcsolat van, a $\lambda_1$ sajátértéket elnevezte algebrai összefüggõségnek. Megmutatta, hogy nem teljes gráfra 
\[\lambda_1\leq e(G).\]

A fenti bizonyítások alapján megsejthetjük, hogy hogyan keressünk optimális bipartíciót. A csúcsokat a $\lambda_1$ sajátvektorhoz tartozó sajátvektor szerint sorba rendezve megkeressük a sorrendben az optimális vágást. Ez közelíteni fogja a tényleges optimumot. Ezt a sajátvektort Fiedler-vektornak hívjuk. \label{Fiedler}

\begin{defin}[Fiedler-vektor]
A $\wg$ gráf normalizált Laplace-mátrixának legkisebb pozitív sajátértékhez tartozó
sajátvektort Fiedler-vektornak hívjuk.
\end{defin}

\subsection{Izoperimetrikus feladatok}
Kézenfekvõ, hogy minimális vágással osszuk részekre a csúcshalmazt.
Azonban könnyen elõfordulhat, hogy egy minimális vágás csak egy csúcsot vág le a gráfról, valójában pedig nem ez a ,,legjobb'' vágás.

\begin{figure}[h]
\centering
\includegraphics[scale=1]{pictures/mincut.jpg} 
\caption{Példa arra, amikor a minimális vágás nem optimális eredményhez vezet}
\end{figure}

Keresnünk kell valamilyen más mértéket, ami mentén optimalizálunk.
Azokat a problémákat, ahol optimális vágásokat keresünk a halmazok méretének figyelembe vételével,
izoperimetrikus problémáknak nevezik.

Célunk, hogy az egy partícióba tartozó csúcsok hasonlóak legyenek. Ennek eléréshez optimalizálhatunk arra, hogy az egy partícióba levõ csúcsok ne legyenek túl messze egymástól. Másként megfogalmazva, hogy az egyes részek átmérõje ne legyen túl nagy.

\begin{defin}[Átmérõ]
Egy gráf átmérõjén a csúcsai között fellépõ távolságok maximumát értjük.   
\[D(G) = \max_{u,v\in V(G)} dist(u,v)\]
\end{defin}
Tehát van értelme vizsgálni a következõ feladatot:
\[\min_{P_k\in \mathcal{P}_k}\max_{V_i \in P_k} D(V_i).\]
Ez a példa jól demonstrálja azt az esetet, amikor az átmérõvel nem a  kívánt partíciókat kapjuk.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.1]{pictures/diameter-fail.jpg}
\caption{Példa arra, amikor a max. átmérõ nem ad optimális vágást. Forrás:\cite{kannan2004clusterings}}
\end{figure}

Szeretnénk megtartani a minimum vágás azon tulajdonságát, hogy a komponensek közötti élek összsúlya kicsi.
Így mértékek meghatározásához érdemes a minimum vágásból kiindulni, és normálni a csúcshalmazok valamilyen mérõszámával.

Legegyszerûbb a csúcshalmazok elemszámával normálni, így kapjuk a kiegyensúlyozottság mértékét.
\begin{defin}[Kiegyensúlyozottság]
\[b(P_k, G) = \sum_{a=1}^{k-1} \sum_{b=a+1}^{k} \left (\frac{1}{|V_a|} + 
\frac{1}{|V_b|}\right )w(V_a,V_b)=\]
\[=\sum_{a=1}^k\frac{w(V_a, \overline{V_a})}{|V_a|} =
 k-\sum_{a=1}^k\frac{w(V_a,V_a)}{|V_a|}\]
\end{defin}

Normálhatunk az illeszkedõ élek súlyával is.
Késõbb is hasznos lesz a következõ fogalom.

\begin{defin}[Csúcshalmaz térfogata]
A $\graf$ gráf $S\subset V$ csúcshalmazának térfogata:
\[Vol S = \sum_{v\in S}d_v.\]
\end{defin}

Shi és Malik \cite{shi2000normalized} az alábbi mértéket javasolja. 

\begin{defin}[Normalizált k-vágás érték]
\[Ncut(P_k, G)=\sum_{a=1}^{k-1}\sum_{b=a+1}^{k}
\left (\frac{1}{Vol(V_a)}+\frac{1}{Vol(V_b)}\right )w(V_a,V_b)=\]
\[=\sum_{a=1}^k\frac{w(V_a, \overline{V_a})}{Vol(V_a)} =
 k-\sum_{a=1}^k\frac{w(V_a,\overline{V_a})}{Vol(V_a)}\]
\end{defin}

A kiterjesztés fogalma is gyakran használatos az optimális vágás meghatározásához. Azt fejezi ki, hogy mennyivel növekszik egy gráf, ha hozzávesszük az adott csúcshalmazt.
\begin{defin}[Kiterjesztés]
\[\Psi(S)=\frac{w(S,\overline{S})}{\min(|S|, |\overline{S}|)}\]
\end{defin}

A kiterjesztés hiányossága, hogy minden csúcsot egyformán súlyoz. Ezt kijavíthatjuk, ha nem elemszámot, hanem térfogatot veszünk alapul. Így a konduktancia fogalmához jutunk.

\begin{defin}[Konduktancia]
\[\Phi(S)=\frac{w(S,\overline{S})}{\min(Vol(S), Vol(\overline{S}))}\]
\end{defin}

\subsection{Bipartíciók}
Elõször azzal a kérdéssel foglalkozunk, amikor biparticionálni szeretnénk $G$-t.


\subsubsection{A Cheeger-konstans}
Tekintsük a következõ feladatot!

\textbf{1. feladat:} Egy fix $m$-re keressük azt az $S$ csúcshalmazt,
amire $m\leq Vol(S)\leq Vol(\overline{S})$ és $w(\partial S)$ minimális.

A választ az ún. \textit{Cheeger-konstans} adja meg.

\begin{defin}[Cheeger-konstans]
Egy $\graf$-hoz tartozó Cheeger-konstans
\[h_G=\min_{S\subset V}\Phi\left(S\right).\]
\end{defin}
$h_G > 0$ csak akkor igaz, ha $G$ összefüggõ, ezért feltesszük, hogy $G$ összefüggõ.

Az 1. feladat megoldása egyenértékû a Cheeger-konstans meghatározásával, mivel
\[w(\partial S)\geq h_G Vol(S).\quad\cite{chung1997spectral}\]


\subsubsection{A Cheeger-egyenlõtlenség}
A következõkben a Cheeger-konstans és a spektrum kapcsolatát fogjuk vizsgálni.
\begin{tetel}[Cheeger-egyenlõtlenség]
Egy $\wg$ összefüggõ élsúlyozott gráfhoz tartozó $h_G$ Cheeger-konstansra és a spektrumának legkisebb pozitív sajátértékére igazak az alábbi egyenlõtlenségek.
\[\frac{h_G^2}{2}<\lambda_1 \leq 2 h_G\].
\end{tetel}
\begin{biz}[Felsõ becslés] Legyen $S\subset V$ Cheeger-optimális. vagyis $h_G\left( S\right) = h_G$. Tekintsük a következõ $f:V\rightarrow\mathbb{R}$ függvényt:
\[ f\left(v \right) =
\left\{
\begin{array}{cc}
\frac{1}{Vol (S)}&\textrm{ha } v \in S\\
\frac{1}{Vol( \overline{S})}& \textrm{ha } v \in \bar{S}\\
\end{array}
\right.
\]
$f$-t behelyettesítve \eqref{Rayleigh}-ba kapjuk a következõt:
\[
\lambda_1 \leq \partial S\left(\frac{1}{Vol(S)}+\frac{1}{Vol(\overline{S})}\right) \leq
\frac{2\partial S}{\min(Vol (S), Vol (\overline{S}))} =
2 h_G
.\]
\end{biz}
A másik irányt az egyszerûség kedvéért most csak abban az eseteben látjuk be, amikor nem súlyozottak az élek.

\begin{biz}[Alsó becslés]
Nézzük a $\lambda_1$-hez tartozó egységhosszú sajátvektort, $f$-t és $g=D^{-1/2}f$  vektorokat,
mint $V\rightarrow \mathbb{R}$ függvényeket. Mivel $f$ merõleges a $\sqrt{\underline{d}}$-re, ezért igaz, hogy
\begin{equation}
\label{eq:gd0}
\sum_v g(v)d_v=0.
\end{equation}
%\[\sum_v g(v)^2 d_v=1.\]
Feltehetõ, hogy a csúcsok $f$ szerint csökkenõ sorrendbe vannak rendezve vagyis $f(v_i) \geq f(v_j)\textrm{, ha } i<j$. Legyen $S_i=\{v_1,\dots, v_i\}$, valamint \[\alpha_G=\min_i h_{S_i}.\].
Legyen $r$ a legnagyobb egész, amire $Vol(S_r)<Vol(G)/2$. \eqref{eq:gd0} miatt
\[\sum_v g(v)^2d_v=\min_c\sum_v(g(v)-c)^2 d_v\leq \sum_v(g(v)-g(v_r))^2d_v.\]
Jelölje $g_+$ és $g_-$ $g$ pozitív ill. negatív részét.
\[\lambda_1 = \frac{\sum_{u\sim v}(g(u)-g(v)^2)}{\sum_v g(v)^2d_v}
\geq \frac{\sum_{u\sim v}(g(u)-g(v)^2)}{\sum_v (g(v)-g(v_r))^2d_v}\]
\[\geq\frac{\sum_{u\sim v}\Big(\big(g_+(u)-g_+(v))^2 + (g_-(u)-g_-(v)\big)^2\Big)}
{\sum_v(g_+(v)^2+g_-(v)^2)d_v}\]
Mivel
$
\frac{a+b}{c+d} \geq min(\frac{a}{c},\frac{b}{d})
$, feltehetõ, hogy
\[\lambda_1 \geq \frac{\sum_{u\sim v}(g_+(u)-g_+(v)^2)}{\sum_v g_+(v)^2d_v}\]

\[=\frac{\Big(\sum_{u\sim v}\big(g_+(u)-g_+(v)\big)^2\Big)
\Big(\sum_{u\sim v}\big(g_+(u)+g_+(v)\big)^2\Big)}
{\Big(\sum_v g_+(v)^2d_v\Big)\Big(\sum_{u\sim v}\big(g_+(u)+g_+(v)\big)^2\Big)}\]

\[\geq \frac{\Big(\sum_{u\sim v}\big(g_+(u)^2-g_+(v)^2\big)\Big)^2}{2\left (\sum_v g_+^2(v)d_v\right )^2}\textrm{, a Cauchy-Schwarz-Bunyakovszkij egyenlõtlenség miatt,}\]
\[=\frac{\Big(\sum_i |g_+(v_i)^2-g_+(v_{i+1})^2|\,|\partial(S_i)|\Big)^2}{2\Big(\sum_v g_+(v)^2d_v\Big)^2}\]
Bevezetve a
\[\tilde{Vol}(S)=\min(Vol(S), Vol(G)-Vol(S))\] 
jelölést kapjuk továbbá, hogy
\[\lambda_1 \geq \frac{\Big(\sum_i |g_+(v_i)^2-g_+(v_{i+1})^2|\alpha_G|\tilde{Vol}(S_i)|\Big)^2}{\Big(\sum_v g_+(v)^2d_v\Big)^2}\textrm{, }\alpha_g\textrm{ definíciója miatt}\]
\[=\frac{\alpha_g^2}{2}\frac{\Big(\sum_i g_+(v_i)^2\big(|\tilde{Vol}(S_i)-\tilde{Vol}(S_{i+1})|\big)\Big)^2}{\Big(\sum_v g_+(v)^2d_v\Big)^2}\]
\[=\frac{\alpha_g^2}{2}\frac{\Big(\sum_ig_+(v_i)^2d_{v_i}\big)^2}{\Big(\sum_v g_+(v)^2d_v\Big)^2} = \frac{\alpha_G^2}{2}.\]

Beláttuk tehát, hogy $\lambda_1 \geq h_G^2/2$. Ahhoz, hogy belássuk, hogy egyenlõség nem állhat fent szükségünk lesz arra, hogy $1-\sqrt{1-x^2}>x^2/2$, ha $x > 0$, valamint az alábbi lemmára.
\begin{lemma}
Ha $G$ összefüggõ gráf és $\lambda_1$ a spektrumának legkisebb eleme, akkor
\[\lambda_1\geq 1-\sqrt{1-h_G^2} \Rightarrow 
\]
\end{lemma}
\begin{biz}
Láttuk, hogy 
\[\lambda_1 \geq \frac{\sum_{u\sim v}(g_+(u)-g_+(v)^2)}{\sum_v g_+(v)^2d_v} = W.\]
Valamint azt is tudjuk már, hogy
\[W =\frac{\Big(\sum_{u\sim v}\big(g_+(u)-g_+(v)\big)^2\Big)
\Big(\sum_{u\sim v}\big(g_+(u)+g_+(v)\big)^2\Big)}
{\Big(\sum_v g_+(v)^2d_v\Big)\Big(\sum_{u\sim v}\big(g_+(u)+g_+(v)\big)^2\Big)}\]
\[\geq \frac{\Big(\sum_{u\sim v}|g_+(u)^2-g_+(v)^2|\Big)^2}
{\Big(\sum_vg_+(v)d_v\Big)\Big(2\sum_vg_+(v)^2d_v-W\sum_vg_+(v)^2d_v\Big)}\]
\[\geq\frac{\Big(\sum_i |g_+(v_i)^2-g_+(v_{i+1})^2|\,|\partial S_i|\Big)^2}
{(2-W)\Big(\sum_v g_+(v)^2\Big)^2d_v}\]
\[\geq\frac{\Big(\sum_i\big(g_+(v_i)^2-g_+(v_{i+1})^2\big)\alpha_G\sum_{j\leq i}d_j\Big)^2}
{(2-W)\Big(\sum_v g_+(v)^2\Big)^2d_v}\]
\[
\geq \frac{\alpha_G^2}{2-W}
\]
Ebbõl következik, hogy
\[0\geq W^2-2W+\alpha_G^2.\]
A másodfokú egyenlet megoldóképletét alkalmazva kapjuk a kívánt egyenlõtlenséget.
\[\lambda_1 \geq W \geq 1-\sqrt{1-\alpha_G^2} \geq 1 - \sqrt{1-h_G^2}\]
\end{biz}
A lemma bizonyításával beláttuk a Cheeger-egyenlõtlenség másik irányát is. \cite{chung1997spectral}
\end{biz}

A $k$-dimenziós kocka Cheeger-konstansa $2/n$, ugyanannyi, mint a spektrumának $\lambda_1$ eleme. Tehát a Cheeger-egyenlõtlenség felsõ becslése konstans szorzó erejéig pontos. \\
A $n$-hosszú út spektrumának elsõ pozitív eleme $1-\cos\frac{\pi}{n-1}\approx\pi^2/2(n-1)^2$,
a Cheeger-konstansa pedig $1/\lfloor \frac{n-1}{2} \rfloor$.
Megmutattuk tehát, hogy a Cheeger-egyenlõtlenség alsó iránya is a lehetõ legjobb becslés konstans szorzó erejéig.

\subsection{Klaszterezés}
A következõkben áttérünk a multipartíciók vizsgálatára.

\subsubsection{Reprezentánsok keresése QP programmal}

A klaszterezést megnehezíti adataink sokdimenziós volta. Ezért érdemes az egyes csúcsokat kisdimenziós vektorokkal reprezentálni.
Ehhez tekintsük a következõ kvadratikus programozási feladatot!

Adott $\wg$ gráfhoz és adott $1\leq k \leq n$ egészhez keresünk olyan $k$-dimenziós $r_1, \dots r_n$ reprezentáns vektorokat, hogy a 
\begin{equation}
Q_k = \sum_{i < j} w_{ij} \norma{r_i - r_j} \geq 0 \label{celfuggveny}
\end{equation}
célfüggvény érték minimális legyen a
\begin{equation}
\sum_{i=1}^n r_ir_i^T = I_k \label{feltetelek}
\end{equation}
feltétel mellett ($I_k$ jelöli a $k$ dimenziós egységmátrixot).
Látható, hogy a célfüggvény értéke csökken, ha a nagy hasonlóságú csúcsok közelebb kerülnek egymáshoz.

Átalakítjuk a célfüggvényt és a feltéteket egy többet mondó formába.
Legyen $X$ az a mátrix, aminek a sorai $r_1^T,\dots ,r_n^T$. Legyenek $x_1, \dots, x_k \in \mathbb{R}^n$ $X$ oszlopai.
A \eqref{feltetelek} feltételek miatt $X$ oszlopai ortonormált rendszert alkotnak, tehát $X$ egy szubortogonális mátrix.
Így a feltételeket felírhatjuk $X^TX=I_k$ formában is. Ezek után a célfüggvényt tovább alakíthatjuk (\ref{Qk}) egyenlet szerint.
\begin{eqnarray}
\min \nyom{X^TLX} \label{minQk} \\
X^TX=I_k
\end{eqnarray}

\begin{tetel}[Reprezentációs tétel hasonlósággráfokra]
Tekintsük a $G=(V,W)$ összefüggõ gráfot és a hozzá tartozó $L$ Laplace-mátrixot!
Jelölje $0=\lambda_0\leq\lambda_1\leq\cdots\leq\lambda_{n-1}$ az $L$ sajátértékeit,
valamint $u_0,u_1,\dots,u_{n-1}$ a hozzájuk tartozó
egységhosszú sajátvektorokat! Legyen $k<n$ olyan,
hogy $\lambda_{k-1}<\lambda_k$!
Ekkor a \eqref{celfuggveny} célfüggvény értéke a \eqref{feltetelek} feltételek mellett:
\[\sum_{i=0}^{k-1}\lambda_i=\sum_{i=1}^{k-1}\lambda_i.\]
Az optimális reprezentánsok $r_1^*,\dots,r_n^*$ az $X^*=\left(u_0, u_1, \dots ,u_{k-1}\right)$ mátrix sorainak transzponáltjai.
\end{tetel}
\begin{biz}
A \eqref{minQk} alak szerint:
\[ Q_k = \nyom{X^TLX}.\]
Így a tétel következik az alábbi általánosabb állításból.
\begin{allitas}
Legyen $A$ egy valós $n\times n$ méretû szimmetrikus mátrix. $A$ sajátértékei $\lambda_1\geq\dots\geq\lambda_n$.
$k > 0$ olyan, hogy $\lambda_k > \lambda{k+1}$ Ekkor
\[
\max_{X\in\mathbb{R}^{n\times k}\atop{ X^TX=I_k}} \nyom{X^TAX}=
\max_{x_i \in \mathbb{R}^n \left( i=1,\dots ,k \right)\atop{ x_i^Tx_j=\delta_{ij}}}
\sum_{i=1}^k x_i^T A x_i = \sum_{i=1}^k\lambda_i
.\]
Az optimum az $X=\left(u_1,\dots,u_k\right)$ szubortogonális mátrix, ahol $u_i$ az $\lambda_i$-hez tartozó egységhosszú sajátvektor. \end{allitas}
\end{biz}\cite{bolla}

A klaszterek meghatározásához alkalmazhatunk általános geometriai klaszterezõ algoritmust a jóval kisebb dimenziós reprezentáns vektorokon.
Most bemutatjuk a legismertebb ilyen klaszterezõ eljárást, a k-means algoritmust.

\textbf{k-means algoritmus}

Adott $l$ db $\mathbb{R}^n$-beli vektor, és $i$ iterációszám.
\begin{enumerate}
\item Választunk $k$ db tetszõleges klaszterközéppontot, $\emph{számláló} := 0$.
\item Minden csúcsot beosztunk abba a klaszterbe, amelyiknek a középpontja legközelebb esik hozzá.
\item Minden klaszter új középpontja legyen a benne levõ pontok átlaga!
\item Ha $\emph{számláló} = i$, akkor megállunk, egyébként visszalépünk 2. pontra és \emph{számláló}-t növeljük 1-gyel. 
\end{enumerate}

\subsubsection{Spektrálklaszterezés}
Már láttuk a \ref{Fiedler} fejezetben, hogy a Fiedler-vektor segítségével hogyan határozhatunk meg vágásokat.
Klaszterezés esetén alkalmazhatjuk szubrutinként a vágási eljárást.

\textbf{Hagen algoritmusa}\cite{hagen1991fast}
\begin{enumerate} 	\label{spektralHeurisztika}
\item Vegyük a $\lambda_1$-hez tartozó sajátvektort
\item A Fiedler-vektor szerint sorba rendezzük a csúcsokat
\item Megkeressük a minimális konduktanciájú vágást a sorrendben
\item A két részre külön-külön alkalmazzuk az algoritmust
\end{enumerate}

Kannan, Vempala és Vetta\cite{kannan2004clusterings} becslést is adtak az
elõzõ algoritmus által talált klaszterek minõségére. A becsléshez az alábbi mérték használták.

\begin{defin}[$(\alpha , \epsilon )$-partíció]
Egy $\{V_1, \dots, V_k\}$ partíció $(\alpha , \epsilon )$-partíció, ha
\begin{itemize}
\item $C_i$ konduktanciája legalább $\alpha$
\item $i\neq j,$ $C_i$ és $C_j$ között menõ élek súlya az összes él súlyának legfeljebb $\epsilon$ része.
\end{itemize}
\end{defin}

\begin{allitas}[Kannan-Vempala-Vetta]
Ha a $G$ gráfnak létezik $(\alpha , \epsilon )$-partíciója,
akkor Hagen algoritmusa talál
\[\left (\frac{\alpha^2}{72\log^2\frac{n}{\epsilon}},
20\sqrt{\epsilon}\log\frac{n}{\epsilon}\right )-\textrm{partíciót}.\]
\end{allitas}

\subsection{Spektrálklaszterezés több sajátvektorral}
Az elõzõ részben látott algoritmus hátránya, hogy nem stabil és nem hatékony.
Shi és Malik \cite{shi2000normalized} egy másik megközelítést javasol a spektrálklaszterezésre. Nem csak a Fiedler-vektort, hanem több sajátvektort is érdemes figyelembe vesznek.
Az általuk használt algoritmus a következõ:

\textbf{Shi és Malik algoritmusa}
\begin{enumerate}
\item Keressük meg a normalizált Laplace-mátrix elsõ $k$ pozitív sajátértékéhez tartozó sajátvektort.
\item A vektorokból képzett mátrix sorait klaszterezzük pl. $k-means$ algoritmussal.
\item A $v$ csúcs a $v$-nek megfelelõ sor klaszterébe kerül.
\end{enumerate}

Shi és Malik megmutatta, hogy minél több sajátértéket veszünk, annál jobban közelítjük az optimális normalizált k-vágást. Brand és Huang\cite{brand2003unifying} megmutatta, hogy több sajátvektort véve a klaszterek eloszlása egyenletesebb lesz, a
hasonló pontok távolsága csökken, a különbözõké pedig növekszik.

\section{Spektrálklaszeterezés a gyakorlatban}
A következõ fejezetben a spektrálklaszterezés gyakorlatban való alkalmazhatóságát vizsgáljuk meg.
A gyakorlatban azt tapasztaljuk, hogy a nagy gráfokon a spektrumon alapuló algoritmusok gyengébb eredményeket érnek el, mint más megközelítést használó módszerek.
Az alábbiakban ennek okait, valamint néhány lehetséges megoldást mutatunk be egy konkrét példán keresztül.
\subsection{A spektrálklaszterzés egy gyengesége}
\subsubsection{A TKC-hatás}
Lempel és Moran\cite{lempelTKC} az internetgráf vizsgálatakor derítettek fényt  a HITS-algoritmus egy sebezhetõségére. Azt vették észre, hogy a HITS-algoritmus gyakran felértékel a valóságban nem mértékadó oldalakat. 
A hiba okaként a kis sûrû részgráfokat, csomópontokat azonosították. Ezért a
jelenséget TKC-hatásnak \emph{(tightly kit communities effect)} nevezték el.

%TODO Ennek okáról szöveget beírni!
\begin{lemma}
\label{lemma:eigenvector}
Legyen $S = U \Lambda U^T$ szimmetrikus pozitív szemidefinit mátrix, ahol
\[
\Lambda = \left(
\begin{array}{ccc}
\lambda_0 & \ldots & 0 \\
0 & \ddots & 0 \\
0 & \ldots & \lambda_{n-1}
\end{array}
\right).
\]

Ha az $x$ kiinduló vektor merõleges $u_1,\ldots,u_{i-1}$-re, de nem merõleges $u_i$-re,  és $\lambda-{i+1} < \lambda_i$, akkor

\begin{equation}
\lim_{k\mapsto\infty} \frac{S^k x}{\lambda_i^k} = u_i.
\end{equation}
\end{lemma}

\begin{biz} A HITS-nél látott \eqref{hitsEgyenlet} egyenlethez hasonlóan $S^k u_i = \lambda_i^k u_i$. Az  állítás következik, ha felírjuk $x$-t az $u_i$-k, mint bázisok által meghatározott térben.
\end{biz}

\begin{lemma}
\label{lemma:path-count}
Legyen $S$ egy súlyozott gráf  élmátrixa. Ekkor $S^k x$ az $x$-el súlyozott kezdõcsúcsokból induló $k$ hosszú séták  összsúlya.
\end{lemma}

A fenti két lemmát alkalmazhatjuk a HITS algoritmusnál látott $S=AA^T$ mátrixokra, a Laplace, illetve a súlyozott Laplace mátrixokra is. A \ref{lemma:path-count}.~Lemma szerint azokra a $j$ csúcsokra lesz magas az $[S^k x]_j$  értéke, amelyekbõl nagyon sok súlyozott út indul. A \ref{lemma:eigenvector}.~Lemma szerint ezeknek a $j$ csúcsoknak lesz kiemelkedõ  értéke az elsõ olyan sajátvektorban, amely $x$-re nem merõleges.

\subsection{Lehetséges módszerek a TKC-hatás semlegesítésére}
Ebben a részben néhány heurisztikát szeretnénk ismertetni,
melyek segítségével javítható a spektrálklaszterezés eredménye.
\subsubsection{Csomópontok eltávolítása}
A TKC-hatást okozó csomópontok megtalálására alkalmazhatjuk a SCAN-algoritmust\cite{xu2007scan}. Ha megtaláltuk a gráf méretéhez képest elhanyagolható csomópontokat, azokat elhagyhatjuk.
\subsubsection{Csápok összehúzása}
Csápoknak neveztük egy elõre rögzített $d_t$ fokszámnál kisebb fokú csúcsokat. Ezek eltávolítására a következõ eljárást alkalmazhatjuk.
\begin{enumerate}
\item{Keresünk egy $v$ csúcsot, amire $d_v < d_t$, ha nincs ilyen, akkor megállunk.}
\item{$v$-t összehúzzuk a legnagyobb fokszámú szomszédjával.}
\item{Felírjuk, hogy melyik két csúcsot húztuk össze.}
\item{Újra kezdjük az 1. lépéstõl.}
\end{enumerate}
Az eljárás véges, mert minden lépésben csökken a csúcsszám.

\subsection{Spektrálklaszterezés egy konkrét példán bemutatva}
Most pedig egy valós példán demonstráljuk, hogy hogyan alkalmazható a spektrálklaszterezés a fenti heurisztikák használatával.
\subsubsection{Az adat: livejournal.com}
Példánkban\cite{kurucz} a LiveJournal(www.livejournal.com) oldal felhasználóit szerették volna klaszterezni.
A LiveJournal egy közösségi blog oldal, vagyis a felhasználók nem csak tartalmakat közölhetnek, hanem lehetõségük van egymással való kapcsolatukat (barátságukat) is kifejezni. A kapcsolat szimmetrikus, így a szociális háló élei irányítatlanok.

Célkitûzés volt, hogy az orosz nyelvû felhasználókat geológia elhelyezkedésük szerint szeparálják. Az orosz nyelvû felhasználók jelentõs része valójában valamelyik környezõ ország lakosa(Ukrajna, Fehéroroszország, Észtország,\dots).

A gráfban 2.379.267 csúcs és 14.286.827 él szerepelt. A felhasználók több mint 80\% amerikai és körülbelül 5\% orosz.

\subsubsection{A kísérlet menete}
A gráfból elõször eltávolították a csomópontokat, majd alkalmazták a csápok összehúzását. Innen kétféleképp haladtak tovább. Egyrészt Shi és Malik algoritmusát követve, másrészt Lang megmutatta, hogy az optimális klaszterezésnek létezik szemidefinit relaxáltja \cite{lang2005fixing}.
Végül visszarakták a kivett csomókat, és a csápokat visszanövesztve kapták a végsõ klasztereket. A spektrálklaszterezést lefuttatták elõfeldolgozás nélkül is, az összehasonlításhoz. 
\subsubsection{Eredmények}
Az eredményeket az alábbi ábrák mutatják be.

\begin{figure}[ht]
\centering
\begin{subfigure}[b]{0.4\textwidth}
	\includegraphics[width=\textwidth]{pictures/klaszter_before.jpg} 
	\caption{Elõfeldolgozás nélkül}
\end{subfigure}
\begin{subfigure}[b]{0.4\textwidth}
	\includegraphics[width=\textwidth]{pictures/klaszter_after.jpg}
	\caption{Elõfeldolgozással}
\end{subfigure}
\caption{A csúcsok eloszlása 4. és 5. sajátirányok mentén}
\end{figure}

Látható, hogy , míg az elõfeldolgozás nélkül a két csoportot nem lehet megkülönböztetni. A heurisztikák alkalmazása után a klaszterek eltávolodnak egymástól.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{pictures/sdp_result.jpg}
\caption{A szemidefinit program kimenete}
\end{figure}

A szemidefinit programmal összehasonlítható lett az eredmény, míg a spektrálklaszterezés sokkal gyorsabban futott.

\section{Összefoglalás}

Jelen dolgozatban betekintést nyújtottunk a spektrálgráfelmélet alapvetõ eredményeibe, bár korántsem érintettünk minden kapcsolódó témakört. A vágásokon felül nem beszéltünk a további sajátértékeket érintõ becslésekrõl, más gráfparaméterekkel való összefüggésekrõl. 
A jelenleg ismertebb véletlengráfmodellek, mint a Barabási-Albert-modell, rosszul modellezik a valóságban létezõ nagy gráfokat a spektrálklaszterezés nehézségeinek tekintetében.


A fentiek tükrében további kutatási területként merül fel olyan egyszerû véletlengráfmodell felállítása, amely jól modellezi a fenti problémát.
Késõbbi kutatásaink során szeretnénk olyan elõfeldolgozási lépéseket találni, amelyekre bizonyítható, hogy a kívánt módon módosítják a spektrumot.


\bibliographystyle{huplain}

\bibliography{hivatkozasok}

\end{document} 
